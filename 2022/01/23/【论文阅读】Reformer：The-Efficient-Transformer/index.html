

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.svg">
  <link rel="icon" href="/img/favicon.svg">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="Bobo">
  <meta name="keywords" content="">
  
  <title>【论文阅读】Reformer：The Efficient Transformer - Bobo&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->

  
<link rel="stylesheet" href="//at.alicdn.com/t/font_2889727_iagjslv6qh.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"imbobo.club","root":"/","version":"1.8.11","typing":{"enable":true,"typeSpeed":100,"cursorChar":"|","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":"30b718b17a21323675e1dd271428f141","google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"4AJakKYV66arv4EU62AtMzWP-gzGzoHsz","app_key":"wbrGfp6CAsTk5yf5pft407aA","server_url":"https://4ajakkyv.lc-cn-n1-shared.com"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Bobo' Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" target="_blank" rel="noopener" href="http://cloud.liujiawei.xyz">
                <i class="iconfont icon-yunpanyunwenjian"></i>
                云盘
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" target="_blank" rel="noopener" href="http://www.liujiawei.xyz">
                <i class="iconfont icon-lianjie"></i>
                小屋
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('https://cdn.jsdelivr.net/gh/WEI-1228/img-hosting@master/20211221/wallhaven-7315m3.1fl1p9c72ke8.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="【论文阅读】Reformer：The Efficient Transformer">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-01-23 14:43" pubdate>
        2022年1月23日 下午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      3.3k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      38
       分钟
    </span>
  

  
  
    
      <!-- LeanCloud 统计文章PV -->
      <span id="leancloud-page-views-container" class="post-meta" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="leancloud-page-views"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">【论文阅读】Reformer：The Efficient Transformer</h1>
            
            <div class="markdown-body">
              <p><strong><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=rkgNKkHtvB">原文🔗</a></strong></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Transformer模型在许多任务上都取得了SOTA的结果，然而训练这些模型通常都需要非常大的开销，特别是对于长文本模型。本文介绍了两种让Transformer模型更加高效的方法。第一，将点积的attention机制用局部敏感哈希来替代（LSH），这让时间复杂度从$O(L^2)$降到了$O(LlogL)$，其中$L$是文本的长度。第二，使用可逆的残差网络代替标准的残差层，这样能只保存一次激活单元，而非一般的N次，其中N为网络的层数。最终得到了模型Reformer，与普通的Transformer模型效果不相上下，但内存开销更小，并且更快。</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>为了解决普通的Transformer内存开销大，时间复杂度高的问题，本文运用了下列技术：</p>
<ul>
<li>可逆层（上一篇论文笔记介绍），能够只存储一层的激活单元</li>
<li>将前馈层的激活单元拆分开来，并且分块处理它们，能降低空间复杂度</li>
<li>基于LSH的近似attention计算将attention部分的时间复杂度从$O(L^2)$降低到了$O(L\text{log}L)$</li>
</ul>
<p>本文主要研究上面的三种方法，并且发现它们对模型训练时的影响几乎可以忽略不计。将激活单元拆分只影响代码实现，每一层的数据在数值上与Transformer是一样的。应用可逆残差层确实改变了模型的结构，但是在实验中发现对模型的影响也是可以忽略不计的。最后，LSH是对attention的改进，也是对模型效果影响最大的地方，桶的数目对模型效果影响很大。经过实验作者发现一组参数既能提高模型的效率，又能得到与原始Transformer模型相似的实验结果。</p>
<h2 id="局部敏感哈希注意力机制"><a href="#局部敏感哈希注意力机制" class="headerlink" title="局部敏感哈希注意力机制"></a>局部敏感哈希注意力机制</h2><p>再让我们先回顾一下标准的attention计算过程。</p>
<h3 id="点积注意力"><a href="#点积注意力" class="headerlink" title="点积注意力"></a>点积注意力</h3><p>Transformer中的标准attention是归一化的点积attention。输入包括$d_k$维的queries和keys以及$d_v$维的values。然后计算每个query与所有keys的点积，除以$\sqrt{d_k}$，然后通过一个$softmax$层，得到values的权重。在计算的时候，对于所有query的attention计算过程是同时进行的，因为可以将计算过程转化为矩阵乘法。将query打包成矩阵$Q$，keys和values矩阵打包成矩阵$K$和$V$，计算过程就是：<br>$$<br>\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V\tag{1}<br>$$</p>
<h3 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h3><p>Transformer并不是使用一个attention，而是使用多个attention，通过投影出$h$组不同的 queries，keys，values，得到$h$个结果，然后拼接起来再进行一次投影，得到最后的结果。</p>
<h3 id="节约内存的注意力机制"><a href="#节约内存的注意力机制" class="headerlink" title="节约内存的注意力机制"></a>节约内存的注意力机制</h3><p>为了计算attention消耗的内存，我们将重点放在attention计算公式(1)上。假设Q，K，V有相同的大小[$batch_size,length,d_model$]</p>
<p>。其中主要的问题在于$QK^T$，它的维度是[$batch_size,length,length$]。在实验中，作者训练了64K长度的模型，就算把batch-size设为1，在32位浮点精度的情况下$64K\times64K$的矩阵会消耗16G内存。这就会让模型变得非常不实用，难以处理长文本。但重要的是，这个$QK^T$矩阵没必要完全存储在内存中，因为可以分开计算每个query的attention值，在内存中每次只计算$\text{softmax}(\frac{q_iK^T}{\sqrt{d_k}})V$，然后反向传播需要用它来计算梯度的时候再重新算一遍就可以了。然而这样的方法并不是非常高效，但空间复杂度与文本长度成正比。</p>
<h3 id="哈希注意力机制"><a href="#哈希注意力机制" class="headerlink" title="哈希注意力机制"></a>哈希注意力机制</h3><p>在LSH中，我们初始化两个tensor，Q=K和V，维度为$[batch_size,length,d_model]$。我们也保持了多头注意力机制并且将重点关注在公式(1)。如前文所述，最大的问题在于$QK^V$这一项的复杂度是平方的。但我们感兴趣的是softmax($QK^T$)。由于softmax的结果取决于输入元素中最大的一项，因此对于query $q_i$ 我们只需要将重点关注在K中与queries最近的keys上。例如，如果K的长度是64K，对于每个$q_i$我们只需要考虑一小部分，比如32个或64个最接近的keys，其他的keys与query点积经过softmax得到的结果几乎为0，可以不用考虑。这样就变得高效多了，然而怎么快速的在keys中找到与query最近的那几个呢？</p>
<h3 id="局部敏感哈希"><a href="#局部敏感哈希" class="headerlink" title="局部敏感哈希"></a>局部敏感哈希</h3><p>我们先看一个LSH简单的示意图：</p>
<p><img src="/2022/01/23/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Reformer%EF%BC%9AThe-Efficient-Transformer/image-20220124145006703.png" srcset="/img/loading.gif" lazyload alt="LSH示意图"></p>
<p>想象一个平面上有一个圆，上面有两个点x、y，将这个平面划分为四个部分（桶）（上图中的四个颜色），然后我们随机将这个圆圈旋转一个角度$\theta_0$，x、y两个点会分别落到各自的部分。第一行三幅图中，x分别落在了0,1,2三个位置，y分别落在了3,2,0三个位置，那么在第一次和第三次旋转后，x和y是落在同一个组里，第二次旋转落在不同组里。这就是局部敏感哈希的原理，下面我们将这个原理应用在向量上。</p>
<p>快速在高维空间找到最近的向量的问题也能通过局部敏感哈希(LSH)解决。我们定义一个哈希函数，能将向量x映射到一个哈希值h(x)，它能让相近的向量映射到相同的哈希值，而距离比较远的向量不能，这样的哈希函数就是一种局部敏感哈希。在上述问题中，我们确实就需要将相近的向量映射到相同的哈希值（桶），并且要尽量让每个桶尽量一样大。如果有了这样的一个哈希函数，我们的问题就解决了。</p>
<p>我们通过随机投影来得到上述局部敏感哈希函数。如果我们要得到b个桶（将上图平面分为b份），那么就固定一个大小为$[d_k,b/2]$的随机矩阵$R$，这个矩阵就相当于随机转动那个圆盘的力量，但转动的是一个向量。定义公式$h(x)=argmax([xR;-xR])$，这个公式就是转动的方法，其中$[u;v]$表示两个向量连接。这个方法就是LSH，这也非常容易通过代码实现。</p>
<h3 id="LSH注意力机制"><a href="#LSH注意力机制" class="headerlink" title="LSH注意力机制"></a>LSH注意力机制</h3><p>了解了LSH的原理，下面就介绍LSH attention。首先将普通的attention机制重写，对于每个query位置$i$：<br>$$<br>o_i=\sum\limits_{j\in\mathcal{P_i}}{\text{exp}(q_i\cdot k_j-z(i,\mathcal{P_i}))}v_j\<br>where\ \mathcal{P_i}={j:i\geq j}\tag{2}<br>$$<br>引入了标记$\mathcal{P_i}$表示位置$i$需要attend的位置，$z$表示被除数的函数（比如softmax中的归一化项）。为了更加清晰，忽略了$\sqrt{d_k}$的缩放。</p>
<p>这个公式实际上就是公式(1)的变形，公式(1)中的softmax拆开就是<br>$$<br>o_i=\sum\limits_{j\in\mathcal{P_i}}\frac{e^{q_i\cdot k_j}}{e^{q_1\cdot k_j}+e^{q_2\cdot k_j}+\cdots+e^{q_n\cdot k_j}}v_j<br>$$<br>然后让$\text{exp}(z(i,\mathcal{P_i}))={e^{q_i\cdot k_j}}{e^{q_1\cdot k_j}+e^{q_2\cdot k_j}+\cdots+e^{q_n\cdot k_j}}$就得到了变形公式(2)。</p>
<p>为了使代码支持batch操作，我们通常在一个大的集合$\widetilde{\mathcal{P}}={0,1,\dots,l}\supseteq{\mathcal{P_i}}$里进行attention操作，但要把不属于$\mathcal{P_i}$的元素mask掉：<br>$$<br>o_i=\sum\limits_{j\in\widetilde{\mathcal{P}}_i}\text{exp}(q_i\cdot k_j-m(j,\mathcal{P_i})-z(i,\mathcal{P_i}))v_j\<br>\text{where}\ m(j,\mathcal{P_i})=<br>\left{<br>\begin{array}{lc}<br>\infty&amp;\text{if}\ j\notin\mathcal{P_i}\<br>0&amp;\text{otherwise}<br>\end{array}<br>\right.<br>\tag{3}<br>$$<br>如果mask了，就相当于在分母上除了一个无穷大，就等于0。</p>
<p>下面我们就能看LSH attention了，我们定义$\mathcal{P_i}$为<br>$$<br>\mathcal{P_i}={j:h(q_i)=h(k_j)}\tag{4}<br>$$<br>也就是$\mathcal{P_i}$包括所有与$i$在同一个桶里的元素。</p>
<p>以上就是哈希attention的原理。然而如果仅仅这样操作还是会有问题的。</p>
<p>经过上述操作后，会得到$b$个桶，一段文本的每个字的向量都会落到一个桶里，但潜在的问题是，每个桶里装的词向量数目是不均匀的，有的桶里多一些有的桶里少一些，甚至有可能某个桶里一个词向量都没有，这就让算法进行batch操作变得困难。为了解决这个问题，作者首先保证$h(k_j)=h(q_j)$，也就是让每个词向量的query和key一定落在同一个桶中，通过设置$k_j=\frac{q_j}{\Vert q_j\Vert}$（之前好像说了将query和key设成一样，不知道这里为什么又这样设，实际上后面的query和key是一样的）。然后将query按桶的序号进行排序，同一个桶内的query向量按照该词在句子中的位置进行排序。这样就定义了一种序号映射，$i\mapsto s_i$，$i$表示词在序列中的位置序号，$s_i$表示排序后词所在的位置序号。将attention矩阵也经过这样排序后，在同一个桶内的词都会聚集在attention矩阵的对角线上，并且是连续的。然后我们就能进行batch操作了，定义一个长度$m$，我们将排序后的query序列进行分块，每一块的长度就是$m$。然后根据之前的定义，定义每个$q_i$对应的<br>$$<br>\widetilde{\mathcal{P}}<em>i={ j:\lfloor \frac{s_i}{m}\rfloor - 1\ \le \lfloor\frac{s_j}{m}\rfloor \le \lfloor \frac{s_i}{m} \rfloor }\tag{5}<br>$$<br>这个$\widetilde{\mathcal{P}}<em>i$表示的是位置i的词能attention to的所有词的集合，其中可能包含了不能attend to的词，需要用mask给去掉。这样一来对于所有的输入向量，我们都能用相同的代码计算attention矩阵了，每个输入向量不同的地方就是mask不同。在实验中，作者设置$m=\frac{2l}{n</em>{buckets}}$，每个桶的平均大小为$l/n</em>{buckets}$，设置m为其两倍，也就相当于左右各分一半，这里作者假设每个桶大小达到平均大小两倍的概率非常小。下图就是LSH attention的全过程。</p>
<p><img src="/2022/01/23/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Reformer%EF%BC%9AThe-Efficient-Transformer/image-20220124174413183.png" srcset="/img/loading.gif" lazyload alt="LSH attention过程与attention矩阵"></p>
<p>首先看左图，第一行是未排序的query=key，然后将每个位置的key分配到一个桶中，每个桶用不同颜色表示。分配完之后先通过桶序号进行排序，得到第三行的结果。然后将这段序列进行分块，每块大小为$m$。最后进行在$\widetilde{\mathcal{P}}_i$内进行attention操作。</p>
<p>接着是右图，我们看到a图是标准的attention矩阵，非常稀疏但是没有好好利用这些稀疏。b图是经过桶操作并且排序后的结果。c图让Q=K。d图进行分块操作。</p>
<h3 id="多轮LSH注意力"><a href="#多轮LSH注意力" class="headerlink" title="多轮LSH注意力"></a>多轮LSH注意力</h3><p>由于hash操作是随机的，难免会出现错误情况，但出错的概率能通过多次hash操作减小。假设我们进行$n_{rounds}$轮不同的hash操作${h^{1},h^{2},h^{3},\dots,h^{n_{rounds}} }$，那么最能得到：<br>$$<br>\widetilde{\mathcal{P}}<em>i=\mathop{\cup}\limits</em>{r=1}^{n_{rounds}}\mathcal{P}_i^{(r)}\<br>\text{where} \ \mathcal{P}^{(r)}={ j:h^{(r)}(q_i)=h^{(r)}(q_j) }\tag{6}<br>$$</p>
<h2 id="可逆Transformer"><a href="#可逆Transformer" class="headerlink" title="可逆Transformer"></a>可逆Transformer</h2><p>上面的内容解决了Transformer模型的注意力机制复杂度高问题，这里的方法会进一步节省模型的内存消耗。可逆残差网络在上一篇笔记中已经介绍过了。实际上我是为了读懂这里的方法才去看那篇论文的，于是先写了那篇论文笔记作为铺垫。如果没看的话可以先看上一篇论文方法再来继续看这。我就不再解释一遍revnet的方法了，这里实际就直接将revnet的思想运用到了Transformer模型的残差结构中。我将公式再贴一遍，前向传播：<br>$$<br>y_1=x_1+\mathcal{F}(x_2)\<br>y_2=x_2+\mathcal{G}(y_1)\tag{7}<br>$$<br>反向传播：<br>$$<br>x_2=y_2-\mathcal{G}(y_1)\<br>x_1=y_1-\mathcal{F}(x_2)\tag{8}<br>$$<br>将$\mathcal{F}$换成$\text{Attention}$，$\mathcal{G}$换成$\text{FeedForward}$就得到了可逆的Transformer，作者将归一化层放到了残差结构中去，也就是$\mathcal{F,G}$中。</p>
<h2 id="分块"><a href="#分块" class="headerlink" title="分块"></a>分块</h2><p>由于前馈网络的每个激活单元是互不影响的，因此将输入的序列分块计算，计算完成后再合起来。假设我们将序列分成了$c$块，那么输出为：<br>$$<br>Y_2=[Y_2^{(1)};\dots;Y_2^{(c)}]=[X_2^{(1)}+\text{FeedForward}(Y_1^{(1)});\dots;X_2^{(c)}+\text{FeedForward}(Y_1^{(c)})]\tag{9}<br>$$<br>事实上我感觉这对模型还是有修改的，因为这样就相当于每块共用了前馈层的参数，原始的Transformer每一块的参数是不同的，能表示更多信息，这样做就有信息损失了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇论文结合了Transformer的信息表示优点与RevNet的节省内存的优点以及LSH方法的速度优点，诞生了Reformer。</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a>
                    
                      <a class="hover-with-bg" href="/tags/Transformer/">Transformer</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/04/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Linear-Transformer/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">【论文阅读】Linear Transformer</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/01/02/2021%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/">
                        <span class="hidden-mobile">2021年度总结</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"RQM64KPsC8Stf23bb41EnQx5-gzGzoHsz","appKey":"A7gSBbMzN8gDs9qoI84fYzJ8","placeholder":"说点什么","path":"window.location.pathname","avatar":"retro","meta":["nick","mail","link"],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"requiredFields":[]},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <div style="font-size: 0.85rem"> <span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span> <script src="/js/duration.js"></script> </div> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- LeanCloud 统计PV -->
        <span id="leancloud-site-pv-container" style="display: none">
            总访问量 
            <span id="leancloud-site-pv"></span>
             次
          </span>
      
      
        <!-- LeanCloud 统计UV -->
        <span id="leancloud-site-uv-container" style="display: none">
            总访客数 
            <span id="leancloud-site-uv"></span>
             人
          </span>
      

    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js" ></script>



  <script  src="/js/local-search.js" ></script>




  <script defer src="/js/leancloud.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js" ></script>

  








  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?30b718b17a21323675e1dd271428f141";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":0.8,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"superSample":2,"width":150,"height":300,"position":"left","hOffset":0,"vOffset":-60},"mobile":{"show":true,"scale":0.5},"log":false});</script></body>
</html>
