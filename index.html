

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.svg">
  <link rel="icon" href="/img/favicon.svg">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="(｡･∀･)ﾉﾞ嗨，你也是从隔壁来的吗？！">
  <meta name="author" content="Bobo">
  <meta name="keywords" content="个人博客">
  
  <title>隔壁小店</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->

  
<link rel="stylesheet" href="//at.alicdn.com/t/font_2889727_iagjslv6qh.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"blog.liujiawei.xyz","root":"/","version":"1.8.11","typing":{"enable":true,"typeSpeed":100,"cursorChar":"|","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":"30b718b17a21323675e1dd271428f141","google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"4AJakKYV66arv4EU62AtMzWP-gzGzoHsz","app_key":"wbrGfp6CAsTk5yf5pft407aA","server_url":"https://4ajakkyv.lc-cn-n1-shared.com"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 100vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>隔壁小店</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" target="_blank" rel="noopener" href="http://cloud.liujiawei.xyz">
                <i class="iconfont icon-yunpanyunwenjian"></i>
                云盘
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" target="_blank" rel="noopener" href="http://www.liujiawei.xyz">
                <i class="iconfont icon-lianjie"></i>
                小屋
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('https://cdn.jsdelivr.net/gh/WEI-1228/img-hosting@master/20211221/wallhaven-z8dg9y.1okp56vktkbk.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="木叶飞舞之处，🔥亦生生不息">
              
            </span>

            
          </div>

          
            <div class="scroll-down-bar">
              <i class="iconfont icon-arrowdown"></i>
            </div>
          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      <div class="container nopadding-x-md">
        <div class="py-5" id="board"
          style=margin-top:0>
          
          <div class="container">
            <div class="row">
              <div class="col-12 col-md-10 m-auto">
                


  <div class="row mx-auto index-card">
    
    
    <article class="col-12 col-md-12 mx-auto index-info">
      <h1 class="index-header">
        
        <a href="/2022/04/24/Luong-Attention%E4%B8%8EBahdanau-Attention/" target="_self">
          Luong-Attention与Bahdanau-Attention
        </a>
      </h1>

      <p class="index-excerpt">
        <a href="/2022/04/24/Luong-Attention%E4%B8%8EBahdanau-Attention/" target="_self">
          
          
            
          
          最近正在看RNN相关内容，看Pytorch教程看到一个seq2seq对话机器人教程，里面用到了Attention，这种attention方法是Luong等人提出来的，因此我称为Luong-Attention。在看论文，网上搜索相关文章的时候，还看到了一个Bahdanau提出的Attention方法，称其为Bahdanau-Attention，我就也把这篇文章阅读了一下，下面分别介绍一下这两种Att
        </a>
      </p>

      <div class="index-btm post-metas">
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-date"></i>
            <time datetime="2022-04-24 10:56" pubdate>
              2022-04-24
            </time>
          </div>
        
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-category"></i>
            
              <a href="/categories/Attention/">Attention</a>
            
          </div>
        
        
          <div class="post-meta">
            <i class="iconfont icon-tags"></i>
            
              <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a>
            
              <a href="/tags/Attention/">Attention</a>
            
          </div>
        
      </div>
    </article>
  </div>

  <div class="row mx-auto index-card">
    
    
    <article class="col-12 col-md-12 mx-auto index-info">
      <h1 class="index-header">
        
        <a href="/2022/04/16/%E4%B8%BA%E4%BB%80%E4%B9%88RNN%E4%B8%AD%E7%9A%84batch-size%E9%83%BD%E5%9C%A8%E7%AC%AC%E4%BA%8C%E7%BB%B4%E5%BA%A6/" target="_self">
          为什么RNN中的batch_size都在第二维度
        </a>
      </h1>

      <p class="index-excerpt">
        <a href="/2022/04/16/%E4%B8%BA%E4%BB%80%E4%B9%88RNN%E4%B8%AD%E7%9A%84batch-size%E9%83%BD%E5%9C%A8%E7%AC%AC%E4%BA%8C%E7%BB%B4%E5%BA%A6/" target="_self">
          
          
            
          
          今天在学习RNN的时候发现Pytorch中关于RNN模型的输入形状都是[seq_length, batch, *]，跟Linear层或Conv层都不一样，网上搜了一圈找到答案。
由于batch的存在是为了让计算机并行计算，而RNN模型的计算流程有些特殊，需要接收到上一个x的输出才能继续计算下一个输出，因此如果我们让batch成为first_dim，内存中的数据就是这样存储的


第一种存储
        </a>
      </p>

      <div class="index-btm post-metas">
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-date"></i>
            <time datetime="2022-04-16 16:55" pubdate>
              2022-04-16
            </time>
          </div>
        
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-category"></i>
            
              <a href="/categories/pytorch%E7%AC%94%E8%AE%B0/">pytorch笔记</a>
            
          </div>
        
        
          <div class="post-meta">
            <i class="iconfont icon-tags"></i>
            
              <a href="/tags/pytorch/">pytorch</a>
            
              <a href="/tags/RNN/">RNN</a>
            
          </div>
        
      </div>
    </article>
  </div>

  <div class="row mx-auto index-card">
    
    
    <article class="col-12 col-md-12 mx-auto index-info">
      <h1 class="index-header">
        
        <a href="/2022/04/06/EM%E7%AE%97%E6%B3%95%E7%9A%84%E5%AF%BC%E5%87%BA/" target="_self">
          EM算法的导出
        </a>
      </h1>

      <p class="index-excerpt">
        <a href="/2022/04/06/EM%E7%AE%97%E6%B3%95%E7%9A%84%E5%AF%BC%E5%87%BA/" target="_self">
          
          
            
          
          本文主要对EM算法进行推导导出，不细讲EM算法的背景与原理，如果想从头开始学EM算法可以看李航老师的统计学习方法，或B站徐亦达老师的EM算法推导，我认为举得例子非常清楚，能很清楚的了解EM算法能干什么，在干什么。
EM算法流程
输入：观测变量数据\(Y\)，隐变量数据\(Z\)，联合概率分布\(P(Y,Z|\theta)\)，条件分布\(P(Z|Y,\theta)\)；
输出：模型参数\(\
        </a>
      </p>

      <div class="index-btm post-metas">
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-date"></i>
            <time datetime="2022-04-06 13:07" pubdate>
              2022-04-06
            </time>
          </div>
        
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-category"></i>
            
              <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
            
          </div>
        
        
          <div class="post-meta">
            <i class="iconfont icon-tags"></i>
            
              <a href="/tags/EM%E7%AE%97%E6%B3%95/">EM算法</a>
            
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
            
          </div>
        
      </div>
    </article>
  </div>

  <div class="row mx-auto index-card">
    
    
    <article class="col-12 col-md-12 mx-auto index-info">
      <h1 class="index-header">
        
        <a href="/2022/04/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Linear-Transformer/" target="_self">
          【论文阅读】Linear Transformer
        </a>
      </h1>

      <p class="index-excerpt">
        <a href="/2022/04/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Linear-Transformer/" target="_self">
          
          
            
          
          原文🔗
Fast
Autoregressive Transformers with Linear Attention
摘要
Transformer已经在多个任务上取得了非常好的成绩，但是由于其复杂度与输入序列长度的二次方成正比，如果输入的是非常长的文本，它的速度会变得非常慢。为了解决这个问题，本文将self-attention表示成核函数特征的线性点积形式，并且充分利用矩阵乘积的相关性质
        </a>
      </p>

      <div class="index-btm post-metas">
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-date"></i>
            <time datetime="2022-04-04 22:28" pubdate>
              2022-04-04
            </time>
          </div>
        
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-category"></i>
            
              <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a>
            
          </div>
        
        
          <div class="post-meta">
            <i class="iconfont icon-tags"></i>
            
              <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a>
            
              <a href="/tags/Transformer/">Transformer</a>
            
          </div>
        
      </div>
    </article>
  </div>

  <div class="row mx-auto index-card">
    
    
    <article class="col-12 col-md-12 mx-auto index-info">
      <h1 class="index-header">
        
        <a href="/2022/01/23/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Reformer%EF%BC%9AThe-Efficient-Transformer/" target="_self">
          【论文阅读】Reformer：The Efficient Transformer
        </a>
      </h1>

      <p class="index-excerpt">
        <a href="/2022/01/23/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Reformer%EF%BC%9AThe-Efficient-Transformer/" target="_self">
          
          
            
          
          原文🔗
摘要
Transformer模型在许多任务上都取得了SOTA的结果，然而训练这些模型通常都需要非常大的开销，特别是对于长文本模型。本文介绍了两种让Transformer模型更加高效的方法。第一，将点积的attention机制用局部敏感哈希来替代（LSH），这让时间复杂度从\(O(L^2)\)降到了\(O(LlogL)\)，其中\(L\)是文本的长度。第二，使用可逆的残差网络代替标准的
        </a>
      </p>

      <div class="index-btm post-metas">
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-date"></i>
            <time datetime="2022-01-23 14:43" pubdate>
              2022-01-23
            </time>
          </div>
        
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-category"></i>
            
              <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a>
            
          </div>
        
        
          <div class="post-meta">
            <i class="iconfont icon-tags"></i>
            
              <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a>
            
              <a href="/tags/Transformer/">Transformer</a>
            
          </div>
        
      </div>
    </article>
  </div>

  <div class="row mx-auto index-card">
    
    
    <article class="col-12 col-md-12 mx-auto index-info">
      <h1 class="index-header">
        
        <a href="/2022/01/02/2021%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/" target="_self">
          2021年度总结
        </a>
      </h1>

      <p class="index-excerpt">
        <a href="/2022/01/02/2021%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/" target="_self">
          
          
            
          
          2021年算是人生中比较重要的一年吧，因为经历了保研等事情，自身成长了许多，并且自己的认识也提升了不少。
从年初开始的各项比赛，到年中的各种夏令营，到年末的迷茫。每个阶段都有着不同的心情与感受。
总的来说，自己还有很多缺点没有改掉

不能计划好当日要做的事情
看手机总是停不下来
晚上总是喜欢熬夜
看书看着看着就会想去看一下手机。
总是因为某些事而开始暴躁
总是说脏话

新的一
        </a>
      </p>

      <div class="index-btm post-metas">
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-date"></i>
            <time datetime="2022-01-02 10:42" pubdate>
              2022-01-02
            </time>
          </div>
        
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-category"></i>
            
              <a href="/categories/%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/">年度总结</a>
            
          </div>
        
        
          <div class="post-meta">
            <i class="iconfont icon-tags"></i>
            
              <a href="/tags/%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/">年度总结</a>
            
          </div>
        
      </div>
    </article>
  </div>

  <div class="row mx-auto index-card">
    
    
    <article class="col-12 col-md-12 mx-auto index-info">
      <h1 class="index-header">
        
        <a href="/2021/12/31/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91The-Reversible-Residual-Network/" target="_self">
          【论文阅读】The Reversible Residual Network
        </a>
      </h1>

      <p class="index-excerpt">
        <a href="/2021/12/31/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91The-Reversible-Residual-Network/" target="_self">
          
          
            
          
          原文🔗
摘要
深度残差神经网络推动了图像分类任务的SOTA，并且让又深又宽的网络性能变得更好了。然而内存消耗变成了网络的瓶颈，因为模型需要保存所有的激活单元，以便反向传播时计算梯度。本文提出了Reversible
Residual
Network（RevNet）,ResNets的一种变体，能将每一层的激活单元通过后面的层构造出来。因此，每一层的绝大多数激活单元都不用存储在内存中，在反向传
        </a>
      </p>

      <div class="index-btm post-metas">
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-date"></i>
            <time datetime="2021-12-31 15:31" pubdate>
              2021-12-31
            </time>
          </div>
        
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-category"></i>
            
              <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a>
            
          </div>
        
        
          <div class="post-meta">
            <i class="iconfont icon-tags"></i>
            
              <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a>
            
              <a href="/tags/Resnet/">Resnet</a>
            
              <a href="/tags/%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96/">内存优化</a>
            
          </div>
        
      </div>
    </article>
  </div>

  <div class="row mx-auto index-card">
    
    
    <article class="col-12 col-md-12 mx-auto index-info">
      <h1 class="index-header">
        
        <a href="/2021/12/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Longformer%EF%BC%9AThe-Long-Document-Transformer/" target="_self">
          【论文阅读】Longformer：The Long-Document Transformer
        </a>
      </h1>

      <p class="index-excerpt">
        <a href="/2021/12/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Longformer%EF%BC%9AThe-Long-Document-Transformer/" target="_self">
          
          
            
          
          原文🔗
摘要
基于Transformer的模型不能很好的处理长文本任务，原因是它的时间空间复杂度是文本长度的二次方。为了解决这个限制，作者提出了新的Transformer模型——Longformer，其复杂度是基于文本长度线性的，因此能更容易的去处理上千个Token或更长的文本。
Longformer的attention机制能完全代替标准attention机制，它结合了局部滑动窗口的att
        </a>
      </p>

      <div class="index-btm post-metas">
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-date"></i>
            <time datetime="2021-12-20 16:18" pubdate>
              2021-12-20
            </time>
          </div>
        
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-category"></i>
            
              <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a>
            
          </div>
        
        
          <div class="post-meta">
            <i class="iconfont icon-tags"></i>
            
              <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a>
            
              <a href="/tags/Transformer/">Transformer</a>
            
          </div>
        
      </div>
    </article>
  </div>

  <div class="row mx-auto index-card">
    
    
    <article class="col-12 col-md-12 mx-auto index-info">
      <h1 class="index-header">
        
        <a href="/2021/12/18/%E3%80%90Pytorch%E7%AC%94%E8%AE%B0%E3%80%91Pytorch%E4%B9%8Bscatter%E5%87%BD%E6%95%B0%E7%94%A8%E6%B3%95/" target="_self">
          【Pytorch笔记】Pytorch之scatter函数用法
        </a>
      </h1>

      <p class="index-excerpt">
        <a href="/2021/12/18/%E3%80%90Pytorch%E7%AC%94%E8%AE%B0%E3%80%91Pytorch%E4%B9%8Bscatter%E5%87%BD%E6%95%B0%E7%94%A8%E6%B3%95/" target="_self">
          
          
            
          
          Pytorch中的函数后加上“_”表示在原始tensor的基础上操作，改变原始tensor，若是torch.的函数，则不会改变原始tensor，而是会生成一个新的tensor，因此torch.scatter()和tensor.scatter_()函数的区别就在于是否改变原tensor。下面我们就通过scatter_()函数来介绍该函数作用。
Tensor.scatter_(dim,
index
        </a>
      </p>

      <div class="index-btm post-metas">
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-date"></i>
            <time datetime="2021-12-18 10:39" pubdate>
              2021-12-18
            </time>
          </div>
        
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-category"></i>
            
              <a href="/categories/pytorch%E7%AC%94%E8%AE%B0/">pytorch笔记</a>
            
          </div>
        
        
          <div class="post-meta">
            <i class="iconfont icon-tags"></i>
            
              <a href="/tags/pytorch/">pytorch</a>
            
          </div>
        
      </div>
    </article>
  </div>

  <div class="row mx-auto index-card">
    
    
    <article class="col-12 col-md-12 mx-auto index-info">
      <h1 class="index-header">
        
        <a href="/2021/11/30/%E5%8D%81%E4%B8%80%E6%9C%88%E8%AE%A1%E5%88%92%E5%8F%8A%E5%AE%8C%E6%88%90%E6%83%85%E5%86%B5/" target="_self">
          十一月计划及完成情况
        </a>
      </h1>

      <p class="index-excerpt">
        <a href="/2021/11/30/%E5%8D%81%E4%B8%80%E6%9C%88%E8%AE%A1%E5%88%92%E5%8F%8A%E5%AE%8C%E6%88%90%E6%83%85%E5%86%B5/" target="_self">
          
          
            
          
          11月1日


打毛线打了四圈

西瓜书——半监督学习计算学习理论没看懂跳过了，半监督学习中的图半监督学习没看懂，其他都大概看懂了

信号与系统继续学习


11月2日


GBDT和XGBoostGBDT看懂了，XGBoost看懂了一点，还有很多细节没搞懂，明天也可以继续深挖细节

信号与系统继续学习有一点没太学明白，明天要看书复习一遍

跑步一公里5分06。。
        </a>
      </p>

      <div class="index-btm post-metas">
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-date"></i>
            <time datetime="2021-11-30 22:25" pubdate>
              2021-11-30
            </time>
          </div>
        
        
          <div class="post-meta mr-3">
            <i class="iconfont icon-category"></i>
            
              <a href="/categories/%E8%AE%A1%E5%88%92/">计划</a>
            
          </div>
        
        
          <div class="post-meta">
            <i class="iconfont icon-tags"></i>
            
              <a href="/tags/%E8%AE%A1%E5%88%92/">计划</a>
            
          </div>
        
      </div>
    </article>
  </div>



  <nav aria-label="navigation">
    <span class="pagination" id="pagination">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/#board">2</a><a class="extend next" rel="next" href="/page/2/#board"><i class="iconfont icon-arrowright"></i></a>
    </span>
  </nav>



              </div>
            </div>
          </div>
        </div>
      </div>
    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <div style="font-size: 0.85rem"> <span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span> <script src="/js/duration.js"></script> </div> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- LeanCloud 统计PV -->
        <span id="leancloud-site-pv-container" style="display: none">
            总访问量 
            <span id="leancloud-site-pv"></span>
             次
          </span>
      
      
        <!-- LeanCloud 统计UV -->
        <span id="leancloud-site-uv-container" style="display: none">
            总访客数 
            <span id="leancloud-site-uv"></span>
             人
          </span>
      

    
  </div>


  
  <!-- 备案信息 -->
  <div class="beian">
    <span>
      <a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">
        赣ICP备2021004183号-1
      </a>
    </span>
    
      
        <span>
          <a
            href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=36102802362535"
            rel="nofollow noopener"
            class="beian-police"
            target="_blank"
          >
            
              <span style="visibility: hidden; width: 0">|</span>
              <img src="/img/police_beian.png" srcset="/img/loading.gif" lazyload alt="police-icon"/>
            
            <span>赣公网安备36102802362535号</span>
          </a>
        </span>
      
    
  </div>


  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  









  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js" ></script>



  <script  src="/js/local-search.js" ></script>




  <script defer src="/js/leancloud.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>












  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?30b718b17a21323675e1dd271428f141";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":0.8,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"superSample":2,"width":150,"height":300,"position":"left","hOffset":0,"vOffset":-60},"mobile":{"show":true,"scale":0.5},"log":false});</script></body>
</html>
