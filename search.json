[{"title":"2021年度总结","url":"/2022/01/02/2021%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/","content":"2021年算是人生中比较重要的一年吧，因为经历了保研等事情，自身成长了许多，并且自己的认识也提升了不少。\n从年初开始的各项比赛，到年中的各种夏令营，到年末的迷茫。每个阶段都有着不同的心情与感受。\n总的来说，自己还有很多缺点没有改掉\n\n不能计划好当日要做的事情\n看手机总是停不下来\n晚上总是喜欢熬夜\n看书看着看着就会想去看一下手机。\n总是因为某些事而开始暴躁\n总是说脏话\n\n新的一年当然是希望把上面的缺点全部改掉，做一个全新的自己。\n当然啦，新的一年也应该有新的一年的目标。\n\n和宝贝更好\n减肥\n看100篇论文，并做笔记\n争取发一篇论文\n机器学习基础打牢，做到手推公式\n\n","categories":["年度总结"],"tags":["年度总结"]},{"title":"testpage","url":"/2022/04/05/testpage/","content":"fdsfs犯得上发射点  十分士大夫ss\n"},{"title":"【Pytorch笔记】Pytorch之scatter函数用法","url":"/2021/12/18/%E3%80%90Pytorch%E7%AC%94%E8%AE%B0%E3%80%91Pytorch%E4%B9%8Bscatter%E5%87%BD%E6%95%B0%E7%94%A8%E6%B3%95/","content":"Pytorch中的函数后加上“_”表示在原始tensor的基础上操作，改变原始tensor，若是torch.的函数，则不会改变原始tensor，而是会生成一个新的tensor，因此torch.scatter()和tensor.scatter_()函数的区别就在于是否改变原tensor。下面我们就通过scatter_()函数来介绍该函数作用。\nTensor.scatter_(dim, index, src, reduce=None) → Tensor\n我们首先通过阅读官方文档来看看这个函数的作用是什么。翻译过来主要意思就是，将src中的元素，按照index的顺序放入dim维度中。下面还举了一个3-D的例子\nself[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n如果你能一眼看出这个是怎么做的，那就可以退出不用看了。这三行表示的是在三个不同维度上对将src写到self函数中的效果。其中最关键的位置是index[i][j][k]，因为它决定了src[i][j][k]放进self中的位置。因此，src每个维度都必须&gt;=index的每个维度，因为每个index[i][j][k]都对应着一个src[i][j][k]。\n我通过二维的例子来进行解释。\nx = torch.zeros((3, 5))# tensor([[0., 0., 0., 0., 0.],#         [0., 0., 0., 0., 0.],#\t\t  [0., 0., 0., 0., 0.]])index = torch.tensor([[1, 0, 2, 1, 1],[0, 1, 1, 0, 2]])# tensor([[1, 0, 2, 1, 1],#         [0, 1, 1, 0, 2]])src = torch.randn((2, 5))# tensor([[-0.4014, -0.7197,  1.1508,  1.1462, -1.0964],#         [ 1.0820, -2.2081,  0.6046,  0.3120, -0.1554]])x = x.scatter_(dim=0, index=index, src=src)# tensor([[ 1.0820, -0.7197,  0.0000,  0.3120,  0.0000],#         [-0.4014, -2.2081,  0.6046,  1.1462, -1.0964],#         [ 0.0000,  0.0000,  1.1508,  0.0000, -0.1554]])\n上面怎么得到的呢？你可以按照下面的方式想，就能很容易直观得到结果。\n我们先将index每个位置上的数与src中每个位置上的数对应起来，如下图\n\n这样，每个index的位置，都对应着相应位置一个src的数。我们先在脑子中建立这样一个对应关系的印象，下面会用到这个对应关系。\n通过给的示例self[index[i][j][k]][j][k] = src[i][j][k]可以发现src中的!dim维度的索引（在这个例子中dim=0，!dim列就是第1维度），与x中对应维度的索引一一对应的，也就是说src第j列的数字，一定是放到x的第j列，那么你将src想成一列一列的数\n\n每一列的数，都要放到x的对应列中去，我们来看结果是不是这样的\n\n可以看到，经过处理后，x的每一列数字，就是对应到src每一列数字上，只是在这一列上的顺序不同。那么这个顺序怎么确定呢？这就要用到我们上面画的对应关系了，src对应位置的index作为索引，放到x对应列上的位置去，就得到了最终结果。也就是说，index的每一列都是作为src对应列的顺序存在的。\n最后我们整合一遍，首先看参数dim=0，说明除了第0维度，在其他维度上，src与x的索引是一致的，具体到我们的例子里来说，就是src的每一列与x的每一列位置是对应的，要将src每一列元素，填充到x的每一列中去。但对应的列的填充顺序是什么呢？当然就是看index上，每一列的数字了。\n下面我们再来举一个dim=1的例子。\nx = torch.zeros((3, 5))# tensor([[0., 0., 0., 0., 0.],#         [0., 0., 0., 0., 0.],#\t\t  [0., 0., 0., 0., 0.]])index = torch.tensor([[0, 1, 3, 2, 4], [1, 3, 4, 0, 2]])# tensor([[0, 1, 3, 2, 4],#         [1, 3, 4, 0, 2]])src = torch.randn((2, 5))# tensor([[ 0.2267, -1.5383, -1.1069, -1.3450, -0.5601],#         [ 0.5058,  1.3454,  0.1888, -0.0366, -1.2225]])x = x.scatter_(dim=1, index=index, src=src)\n大家可以先思考一下这个结果是什么？\n按照刚才的方法，首先由于dim=1，因此对于!dim=1的维度，src与x的索引是一致的，在本例中也就是行，src中每行元素应该填充到x的每一行去，因此，x的第三行肯定是空的，因为src只有两行数。而这两行数应该怎么填充呢？当然就是将index中对应行的数当作索引，把src中的数按照索引放到x中去啦。第一行是[ 0.2267, -1.5383, -1.1069, -1.3450, -0.5601]，索引为[0, 1, 3, 2, 4]，按照索引对第一行排序，0.2267放第0个位置，-1.5383放第1个位置，-1.1069放第3个位置…得到[ 0.2267, -1.5383, -1.3450, -1.1069, -0.5601]，第二行自己算算吧，最后结果就是\ntensor([[ 0.2267, -1.5383, -1.3450, -1.1069, -0.5601],        [-0.0366,  0.5058, -1.2225,  1.3454,  0.1888],        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n","categories":["Pytorch笔记"],"tags":["Pytorch"]},{"title":"【论文阅读】Linear Transformer","url":"/2022/04/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Linear-Transformer/","content":"原文🔗\n Fast Autoregressive Transformers with Linear Attention\n 摘要\nTransformer已经在多个任务上取得了非常好的成绩，但是由于其复杂度与输入序列长度的二次方成正比，如果输入的是非常长的文本，它的速度会变得非常慢。为了解决这个问题，本文将self-attention表示成核函数特征的线性点积形式，并且充分利用矩阵乘积的相关性质，来将复杂度从O(n2)O(n^2)O(n2)降到O(n)O(n)O(n)，nnn为序列的长度。我们证实了该公式能通过一直大大加速自回归transformer的速度迭代方法实现，并且揭露了它们与循环神经网络之间的关系。本文介绍的linear transformer能获得原始transformer的性能，并且在超长文本的自回归预测的速度上要比原始的快4000倍。\n 介绍\nTransformer模型最初是在一篇机器翻译的文章中被提出，并且已经证明了在许多任务上都能取得非常好的成绩。它除了在有大量监督学习样本的任务中非常有效，在没有样本或有限样本的迁移学习任务中也非常有效。\n然而，获得这些好处的同时，也伴随着计算量的问题。主要的瓶颈就在于self-attention计算全局感受野，这个操作的复杂度是O(N)O(N)O(N)。于是在实践中，Transformer的训练非常慢，并且表示文本的长度也是有限的。并且这会打破时间连贯性，让模型很难获得长文本的前后依赖关系。\n最近，学者们都在研究如何增加上下文的长度但不牺牲效率。为此，Child等人提出了attention矩阵的稀疏分解，讲attention的复杂度降低到O(NN)O(N\\sqrt{N})O(NN​)。Kitaev等人进一步用局部敏感哈希（Reformer）将attention的时间复杂度降到了O(NlogN)O(NlogN)O(NlogN)。这些方法都让模型学习长文本成为可能。前面提到的这些方法虽然能让模型在长文本条件下顺利学习，都没有加速自回归的推理。\n在本文中。我们提出了linear transformer模型，能大大节省内存占用，并且复杂度是与文本长度呈线性的。我们通过self-attention的核函数的公式，并且利用矩阵乘积的相关属性来计算self-attention的权重。通过该线性公式，我们还将causal masking表示成了线性时间复杂度，常数空间复杂度。这还揭露了transformers与RNN之间的关系，这能让模型的自回归加速几个数量级。\n我们的评测在图像生成与ASR(automatic speech recognition)上进行，评测证明了linear transformer的性能能达到transformer的水平，并且还让速度提升了三个数量级。\n Linear Transformers\n在本节，我们将我们提出的linear transformer公式化。我们提出将传统attention的softmax形式改成基于特征的点积attention，这样能让模型具有更好的时空复杂度，并且能在线性时间内生成文本序列，类似于循环神经网络。\n Transformers\n我们再来回顾一下Transformer公式。\n假设输入文本长度为NNN，每个单词embedding到FFF维的特征向量，那么输入文本的向量表示为x∈RN×Fx\\in \\mathbb{R}^{N\\times F}x∈RN×F，Transformer是由多层变换T:RN×F→RN×FT :\\mathbb{R}^{N\\times F}\\rightarrow\\mathbb{R}^{N\\times F}T:RN×F→RN×F，T1(⋅),…,TL(⋅)T_1(\\cdot),\\dots,T_L(\\cdot)T1​(⋅),…,TL​(⋅)表示的，\nTl(x)=fl(Al(x)+x)(1)T_l(x)=f_l(A_l(x)+x)\\tag{1}\nTl​(x)=fl​(Al​(x)+x)(1)\nfl(x)f_l(x)fl​(x)表示的就是全连接层，Al(x)A_l(x)Al​(x)表示的是attention层，Al(x)+xA_l(x)+xAl​(x)+x表示的是残差层。Al(x)A_l(x)Al​(x)的计算公式如下：\nQ=xWQ,K=xWk,V=xWV,Al(x)=V′=softmax(QKTD)V(2)Q=xW_Q,\\\\ K=xW_k,\\\\ V=xW_V,\\\\\nA_l(x)=V^{&#x27;}=\\text{softmax}(\\frac{QK^T}{\\sqrt{D}})V\\tag{2}\nQ=xWQ​,K=xWk​,V=xWV​,Al​(x)=V′=softmax(D​QKT​)V(2)\nVVV的计算公式如下\nVi′=∑j=1Nsim(Qi,Kj)Vj∑j=1Nsim(Qi,Kj)(3)V_i^{&#x27;}=\\frac{\\sum^N_{j=1}sim(Q_i,K_j)V_j}{\\sum^N_{j=1}sim(Q_i,K_j)}\\tag{3}\nVi′​=∑j=1N​sim(Qi​,Kj​)∑j=1N​sim(Qi​,Kj​)Vj​​(3)\n 线性attention\n公式(2)是attention的一般定义，并且能用来定义其他种类的attention比如多项式attention或者RBF kernel attention。需要注意的是唯一需要加到simsimsim函数的约束是非负。这样就包括了所有kernel k(x,y):R+k(x,y):\\mathbb{R}_+k(x,y):R+​。\n给定一个用特征表示的核函数ϕ(x)\\phi(x)ϕ(x)，我们能将公式2重写为，\nVi′=∑j=1Nϕ(Qi)Tϕ(Kj)Vj∑j=1Nϕ(Qi)Tϕ(Kj)(4)V_i^{&#x27;}=\\frac{\\sum^N_{j=1}\\phi (Q_i)^T\\phi (K_j)V_j}{\\sum^N_{j=1}\\phi (Q_i)^T\\phi (K_j)}\\tag{4}\nVi′​=∑j=1N​ϕ(Qi​)Tϕ(Kj​)∑j=1N​ϕ(Qi​)Tϕ(Kj​)Vj​​(4)\n我们再将其中的常数提出，利用矩阵的性质进一步化简\nVi′=ϕ(Qi)T∑j=1Nϕ(Kj)VjTϕ(Qi)T∑j=1Nϕ(Kj)(5)V_i^{&#x27;}=\\frac{\\phi (Q_i)^T\\sum^N_{j=1}\\phi (K_j)V_j^T}{\\phi (Q_i)^T\\sum^N_{j=1}\\phi (K_j)}\\tag{5}\nVi′​=ϕ(Qi​)T∑j=1N​ϕ(Kj​)ϕ(Qi​)T∑j=1N​ϕ(Kj​)VjT​​(5)\n注意，特征映射ϕ(⋅)\\phi(\\cdot)ϕ(⋅)作用在矩阵Q,KQ,KQ,K的行方向上。\n从公式(2)我们能看出softmax attention的时空复杂度是O(N2)O(N^2)O(N2)。然而我们提出的linear transformer的时空复杂度是O(N)O(N)O(N)，因为我们能提前一次性计算出∑j=1Nϕ(Kj)VjT\\sum_{j=1}^N\\phi (K_j)V_j^T∑j=1N​ϕ(Kj​)VjT​，∑j=1Nϕ(Kj)\\sum_{j=1}^N\\phi (K_j)∑j=1N​ϕ(Kj​)，之后每次用的时候直接取值就可以了。\n 特征映射与计算代价\n对于softmax attention，乘法和加法的总复杂度是O(N2max(D,M))O(N^2\\text{max}(D,M))O(N2max(D,M))，其中DDD是query、key的维度，MMM是value的维度。但是，对于线性attention，我们先计算维度为CCC的特征映射，然后计算新的values，这样只需要O(NCM)O(NCM)O(NCM)的加法和乘法。\n之前的分析都没有考虑核函数和特征函数的选择。注意，对应于指数核的特征函数是无限维的，这使得精确的softmax注意力的线性化是不可行的。另一方面，例如，多项式核具有精确的有限维特征映射，并且已被证明与指数核或 RBF 核函数同样适用。2 次线性化多项式变换器的计算成本为O(ND2M)O(ND^2M)O(ND2M)。这使得计算复杂度在N&gt;D2N&gt;D^2N&gt;D2时有利。 请注意，这在实践中是正确的，因为我们希望能够处理具有数万个元素的序列。对于我们处理较小序列的实验，我们使用了一个特征变换，该特征变换产生如下定义的正相似度函数\nϕ(x)=elu(x)+1where elu(x)={x,if x≥0α(ex−1),if x&lt;0\\phi(x) =\\text{elu}(x)+1\\\\\n\\text{where}\\ \\text{elu}(x)=\n\\left\\{\n\\begin{aligned}\nx,if\\ x\\ge 0 \\\\\n\\alpha(e^x-1),if\\ x\\lt0 \\\\\n\\end{aligned}\n\\right.\nϕ(x)=elu(x)+1where elu(x)={x,if x≥0α(ex−1),if x&lt;0​\n","categories":["论文笔记"],"tags":["论文笔记","Transformer"]},{"title":"【论文阅读】Longformer：The Long-Document Transformer","url":"/2021/12/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Longformer%EF%BC%9AThe-Long-Document-Transformer/","content":"原文🔗\n 摘要\n基于Transformer的模型不能很好的处理长文本任务，原因是它的时间空间复杂度是文本长度的二次方。为了解决这个限制，作者提出了新的Transformer模型——Longformer，其复杂度是基于文本长度线性的，因此能更容易的去处理上千个Token或更长的文本。\nLongformer的attention机制能完全代替标准attention机制，它结合了局部滑动窗口的attention与任务驱动的全局attention。Longformer的效果在WikiHop数据集和TriviaQA数据集的效果超过了RoBERTa模型，并且达到了新的SOTA。\n作者还提出了基于Longformer的Encoder-Decoder模型（LED），来处理文本生成任务，并且取得了良好的效果。\n Longformer实现细节\n原始的Transformer模型attention部分的的时间与空间复杂度是O(n2)O(n^2)O(n2)，其中nnn是文本的长度。为了解决这个挑战，作者将原始的满的attention矩阵稀疏化，通过一种定义好的“注意力模式（Attention pattern）”，指定输入中哪些位置的token相互关注（attend）。下面介绍具体的注意力模式有哪些。\n滑动窗口\n由于局部的上下文是非常重要的，因此注意力模式在每个token附近使用了固定大小的attention window，然后使用多层这样的模型，就能获得非常大的感知野。其中最顶层能感受到句子的所有位置，并且有能力输出包含句子所有位置信息的整个句子的表示，就像CNN一样。\n将窗口的大小固定为www，每个token能够attend自己两边12w\\frac{1}{2}w21​w个token，其计算复杂度就是O(n×w)O(n\\times w)O(n×w)。由于www远小于nnn，因此复杂度是与句子长度nnn呈线性关系的。在一个具有 lll 层的transformer模型中，最顶层的感受野大小是 l×wl\\times wl×w（假设每一层的www是固定的）。根据不同的应用场景，在每一层使用不同大小的www可能有助于平衡模型的效率和表示能力，若www越大，则每个token需要attend的数量越大，表示能力也越强，但计算复杂度也变大，反之亦然。\n\n膨胀的滑动窗口\n为了进一步扩大感受野但不增加计算量，滑动窗口能被”膨胀“。这与膨胀的CNN非常类似，使窗口有一个大小为 ddd 的膨胀间隔，也就是相当于将原本的连续的窗口，中间增加间隔。假设每一层的www和ddd是固定的，那么感受野的大小就膨胀成了l×d×wl \\times d \\times wl×d×w，这样就能让视野扩大到上万个token，即使 ddd 是个很小的值。\n全局Attention\n在本方法中，滑动窗口和膨胀的attention机制对于学习特定任务的表示还不够灵活。因此，作者在输入的某些预先选定的位置添加了全局attention。重要的是，这些全局的attention是对称的，也就是说：全局attention位置的token会attend句子中的所有token，反过来，句子中的所有token都会attend那些具有全局attention特性的token。\n例如，对于分类任务，全局attention用在了[CLS]的位置上，而对于QA任务，全局attention用在了所有的问题token上。\n由于这一类具有全局attention性质的token相对于nnn来说非常小，而且与nnn是不相关的，因此结合了局部attention和全局attention的复杂度还是O(n)O(n)O(n)。\n\n全局Attention的线性投影\n给定了线性投影矩阵Q,K,VQ,K,VQ,K,V，原始的Transformer模型计算attention分数公式为：\nAttention(Q,K,V)=softmax(QKTdk)VAttention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V\nAttention(Q,K,V)=softmax(dk​​QKT​)V\n在Longformer中，使用了两组投影。第一组Qs,Ks,VsQ_s,K_s,V_sQs​,Ks​,Vs​用来计算滑动窗口的attention分数，第二组Qg,Kg,VgQ_g,K_g,V_gQg​,Kg​,Vg​用来计算全局的attention分数。额外的的那个投影矩阵对不同类型的attention的建模提供了灵活性，并且证明了它对下游任务有着好的表现是非常关键的。Qg,Kg,VgQ_g,K_g,V_gQg​,Kg​,Vg​是用与Qs,Ks,VsQ_s,K_s,V_sQs​,Ks​,Vs​对应的值初始化的。\n 自回归语言模型\n自回归，或者从左到右的的语言模型被宽泛的定义为给定前面的token/characters，预测后续一个token/characters的概率分布。这在自然语言处理中是一个最基本的任务，并且之前很多使用transformer来处理长文本的模型都靠这个任务来评估效果。本文也不例外。\nAttention Pattern\n在自回归语言模型中作者使用了膨胀的滑动窗口矩阵。作者在每一层都使用了不同的窗口大小。具体来说，在底层使用小的窗口大小，并且随着层数上升，增大窗口的大小。这就使得顶层的网络能学到高层的文本表示，因为底层的网络能获取到局部信息。此外，这样做还能平衡模型的效率和表现。\n实验\n作者实验时使用了阶梯式的训练步骤，在训练过程中慢慢增加窗口的大小和输入文本的长度。\n\n Longformer-Encoder-Decoder(LED)\n原始的Transformer是Encoder-Decoder结构的模型，目的是为了解决seq2seq的任务，比如摘要和翻译。然而这些模型都不能很好的接收长文本的输入。\n为了解决长文本的seq2seq任务，作者提出了Longformer的变体，包含了与Transformer一样的encoder,decoder结构，唯一不同的在于将原始encoder中的的full attention改为了Longformer中更高效的local+global的attention模式。decoder对encoder的全部输入和之前的decoder输出使用了full attention矩阵。\n\n 总结\n本文提出了Longformer，一种基于transformer的模型，能够处理长文本，这就使得它能更加方便的处理某些任务，不需要像原始方法一样将长文本截断，也不需要复杂的结构来结合这些截断的文本信息。\nLongformer使用了一种attention机制，能结合全局和局部的信息，并且能与输入的长度呈线性复杂度。Longformer在很多数据集上都取得了SOTA的效果，在WiKiHop和TriviaQA上还超过了Roberta，达到了新的SOTA。最后还提出了LED，一种Longformer的变体，能处理seq2seq任务，并且在arXiv长文本摘要任务上取得了好的效果。\n","categories":["论文笔记"],"tags":["论文笔记","Transformer"]},{"title":"【论文阅读】Reformer：The Efficient Transformer","url":"/2022/01/23/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Reformer%EF%BC%9AThe-Efficient-Transformer/","content":"原文🔗\n 摘要\nTransformer模型在许多任务上都取得了SOTA的结果，然而训练这些模型通常都需要非常大的开销，特别是对于长文本模型。本文介绍了两种让Transformer模型更加高效的方法。第一，将点积的attention机制用局部敏感哈希来替代（LSH），这让时间复杂度从O(L2)O(L^2)O(L2)降到了O(LlogL)O(LlogL)O(LlogL)，其中LLL是文本的长度。第二，使用可逆的残差网络代替标准的残差层，这样能只保存一次激活单元，而非一般的N次，其中N为网络的层数。最终得到了模型Reformer，与普通的Transformer模型效果不相上下，但内存开销更小，并且更快。\n 简介\n为了解决普通的Transformer内存开销大，时间复杂度高的问题，本文运用了下列技术：\n\n可逆层（上一篇论文笔记介绍），能够只存储一层的激活单元\n将前馈层的激活单元拆分开来，并且分块处理它们，能降低空间复杂度\n基于LSH的近似attention计算将attention部分的时间复杂度从O(L2)O(L^2)O(L2)降低到了O(LlogL)O(L\\text{log}L)O(LlogL)\n\n本文主要研究上面的三种方法，并且发现它们对模型训练时的影响几乎可以忽略不计。将激活单元拆分只影响代码实现，每一层的数据在数值上与Transformer是一样的。应用可逆残差层确实改变了模型的结构，但是在实验中发现对模型的影响也是可以忽略不计的。最后，LSH是对attention的改进，也是对模型效果影响最大的地方，桶的数目对模型效果影响很大。经过实验作者发现一组参数既能提高模型的效率，又能得到与原始Transformer模型相似的实验结果。\n 局部敏感哈希注意力机制\n再让我们先回顾一下标准的attention计算过程。\n 点积注意力\nTransformer中的标准attention是归一化的点积attention。输入包括dkd_kdk​维的queries和keys以及dvd_vdv​维的values。然后计算每个query与所有keys的点积，除以dk\\sqrt{d_k}dk​​，然后通过一个softmaxsoftmaxsoftmax层，得到values的权重。在计算的时候，对于所有query的attention计算过程是同时进行的，因为可以将计算过程转化为矩阵乘法。将query打包成矩阵QQQ，keys和values矩阵打包成矩阵KKK和VVV，计算过程就是：\nAttention(Q,K,V)=softmax(QKTdk)V(1)\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\\tag{1}\nAttention(Q,K,V)=softmax(dk​​QKT​)V(1)\n 多头注意力机制\nTransformer并不是使用一个attention，而是使用多个attention，通过投影出hhh组不同的 queries，keys，values，得到hhh个结果，然后拼接起来再进行一次投影，得到最后的结果。\n 节约内存的注意力机制\n为了计算attention消耗的内存，我们将重点放在attention计算公式(1)上。假设Q，K，V有相同的大小[batch_size,length,d_modelbatch\\_size,length,d\\_modelbatch_size,length,d_model]\n。其中主要的问题在于QKTQK^TQKT，它的维度是[batch_size,length,lengthbatch\\_size,length,lengthbatch_size,length,length]。在实验中，作者训练了64K长度的模型，就算把batch-size设为1，在32位浮点精度的情况下64K×64K64K\\times64K64K×64K的矩阵会消耗16G内存。这就会让模型变得非常不实用，难以处理长文本。但重要的是，这个QKTQK^TQKT矩阵没必要完全存储在内存中，因为可以分开计算每个query的attention值，在内存中每次只计算softmax(qiKTdk)V\\text{softmax}(\\frac{q_iK^T}{\\sqrt{d_k}})Vsoftmax(dk​​qi​KT​)V，然后反向传播需要用它来计算梯度的时候再重新算一遍就可以了。然而这样的方法并不是非常高效，但空间复杂度与文本长度成正比。\n 哈希注意力机制\n在LSH中，我们初始化两个tensor，Q=K和V，维度为[batch_size,length,d_model][batch\\_size,length,d\\_model][batch_size,length,d_model]。我们也保持了多头注意力机制并且将重点关注在公式(1)。如前文所述，最大的问题在于QKVQK^VQKV这一项的复杂度是平方的。但我们感兴趣的是softmax(QKTQK^TQKT)。由于softmax的结果取决于输入元素中最大的一项，因此对于query qiq_iqi​ 我们只需要将重点关注在K中与queries最近的keys上。例如，如果K的长度是64K，对于每个qiq_iqi​我们只需要考虑一小部分，比如32个或64个最接近的keys，其他的keys与query点积经过softmax得到的结果几乎为0，可以不用考虑。这样就变得高效多了，然而怎么快速的在keys中找到与query最近的那几个呢？\n 局部敏感哈希\n我们先看一个LSH简单的示意图：\n\n想象一个平面上有一个圆，上面有两个点x、y，将这个平面划分为四个部分（桶）（上图中的四个颜色），然后我们随机将这个圆圈旋转一个角度θ0\\theta_0θ0​，x、y两个点会分别落到各自的部分。第一行三幅图中，x分别落在了0,1,2三个位置，y分别落在了3,2,0三个位置，那么在第一次和第三次旋转后，x和y是落在同一个组里，第二次旋转落在不同组里。这就是局部敏感哈希的原理，下面我们将这个原理应用在向量上。\n快速在高维空间找到最近的向量的问题也能通过局部敏感哈希(LSH)解决。我们定义一个哈希函数，能将向量x映射到一个哈希值h(x)，它能让相近的向量映射到相同的哈希值，而距离比较远的向量不能，这样的哈希函数就是一种局部敏感哈希。在上述问题中，我们确实就需要将相近的向量映射到相同的哈希值（桶），并且要尽量让每个桶尽量一样大。如果有了这样的一个哈希函数，我们的问题就解决了。\n我们通过随机投影来得到上述局部敏感哈希函数。如果我们要得到b个桶（将上图平面分为b份），那么就固定一个大小为[dk,b/2][d_k,b/2][dk​,b/2]的随机矩阵RRR，这个矩阵就相当于随机转动那个圆盘的力量，但转动的是一个向量。定义公式h(x)=argmax([xR;−xR])h(x)=argmax([xR;-xR])h(x)=argmax([xR;−xR])，这个公式就是转动的方法，其中[u;v][u;v][u;v]表示两个向量连接。这个方法就是LSH，这也非常容易通过代码实现。\n LSH注意力机制\n了解了LSH的原理，下面就介绍LSH attention。首先将普通的attention机制重写，对于每个query位置iii：\noi=∑j∈Piexp(qi⋅kj−z(i,Pi))vjwhere Pi=j:i≥j(2)o_i=\\sum\\limits_{j\\in\\mathcal{P_i}}{\\text{exp}(q_i\\cdot k_j-z(i,\\mathcal{P_i}))}v_j\\\\\nwhere\\ \\mathcal{P_i}={j:i\\geq j}\\tag{2}\noi​=j∈Pi​∑​exp(qi​⋅kj​−z(i,Pi​))vj​where Pi​=j:i≥j(2)\n引入了标记Pi\\mathcal{P_i}Pi​表示位置iii需要attend的位置，zzz表示被除数的函数（比如softmax中的归一化项）。为了更加清晰，忽略了dk\\sqrt{d_k}dk​​的缩放。\n这个公式实际上就是公式(1)的变形，公式(1)中的softmax拆开就是\noi=∑j∈Pieqi⋅kjeq1⋅kj+eq2⋅kj+⋯+eqn⋅kjvjo_i=\\sum\\limits_{j\\in\\mathcal{P_i}}\\frac{e^{q_i\\cdot k_j}}{e^{q_1\\cdot k_j}+e^{q_2\\cdot k_j}+\\cdots+e^{q_n\\cdot k_j}}v_j\noi​=j∈Pi​∑​eq1​⋅kj​+eq2​⋅kj​+⋯+eqn​⋅kj​eqi​⋅kj​​vj​\n然后让exp(z(i,Pi))=eqi⋅kjeq1⋅kj+eq2⋅kj+⋯+eqn⋅kj\\text{exp}(z(i,\\mathcal{P_i}))={e^{q_i\\cdot k_j}}{e^{q_1\\cdot k_j}+e^{q_2\\cdot k_j}+\\cdots+e^{q_n\\cdot k_j}}exp(z(i,Pi​))=eqi​⋅kj​eq1​⋅kj​+eq2​⋅kj​+⋯+eqn​⋅kj​就得到了变形公式(2)。\n为了使代码支持batch操作，我们通常在一个大的集合P~=0,1,…,l⊇Pi\\widetilde{\\mathcal{P}}={0,1,\\dots,l}\\supseteq{\\mathcal{P_i}}P=0,1,…,l⊇Pi​里进行attention操作，但要把不属于Pi\\mathcal{P_i}Pi​的元素mask掉：\noi=∑j∈P~iexp(qi⋅kj−m(j,Pi)−z(i,Pi))vjwhere m(j,Pi)={∞if j∉Pi0otherwise(3)o_i=\\sum\\limits_{j\\in\\widetilde{\\mathcal{P}}_i}\\text{exp}(q_i\\cdot k_j-m(j,\\mathcal{P_i})-z(i,\\mathcal{P_i}))v_j\\\\\n\\text{where}\\ m(j,\\mathcal{P_i})=\n\\left\\{\n\\begin{array}{lc}\n\\infty&amp;\\text{if}\\ j\\notin\\mathcal{P_i}\\\\\n0&amp;\\text{otherwise}\n\\end{array}\n\\right.\n\\tag{3}\noi​=j∈Pi​∑​exp(qi​⋅kj​−m(j,Pi​)−z(i,Pi​))vj​where m(j,Pi​)={∞0​if j∈/​Pi​otherwise​(3)\n如果mask了，就相当于在分母上除了一个无穷大，就等于0。\n下面我们就能看LSH attention了，我们定义Pi\\mathcal{P_i}Pi​为\nPi=j:h(qi)=h(kj)(4)\\mathcal{P_i}={j:h(q_i)=h(k_j)}\\tag{4}\nPi​=j:h(qi​)=h(kj​)(4)\n也就是Pi\\mathcal{P_i}Pi​包括所有与iii在同一个桶里的元素。\n以上就是哈希attention的原理。然而如果仅仅这样操作还是会有问题的。\n经过上述操作后，会得到bbb个桶，一段文本的每个字的向量都会落到一个桶里，但潜在的问题是，每个桶里装的词向量数目是不均匀的，有的桶里多一些有的桶里少一些，甚至有可能某个桶里一个词向量都没有，这就让算法进行batch操作变得困难。为了解决这个问题，作者首先保证h(kj)=h(qj)h(k_j)=h(q_j)h(kj​)=h(qj​)，也就是让每个词向量的query和key一定落在同一个桶中，通过设置kj=qj∥qj∥k_j=\\frac{q_j}{\\Vert q_j\\Vert}kj​=∥qj​∥qj​​（之前好像说了将query和key设成一样，不知道这里为什么又这样设，实际上后面的query和key是一样的）。然后将query按桶的序号进行排序，同一个桶内的query向量按照该词在句子中的位置进行排序。这样就定义了一种序号映射，i↦sii\\mapsto s_ii↦si​，iii表示词在序列中的位置序号，sis_isi​表示排序后词所在的位置序号。将attention矩阵也经过这样排序后，在同一个桶内的词都会聚集在attention矩阵的对角线上，并且是连续的。然后我们就能进行batch操作了，定义一个长度mmm，我们将排序后的query序列进行分块，每一块的长度就是mmm。然后根据之前的定义，定义每个qiq_iqi​对应的\nP~i={j:⌊sim⌋−1 ≤⌊sjm⌋≤⌊sim⌋}(5)\\widetilde{\\mathcal{P}}_i=\\{ j:\\lfloor \\frac{s_i}{m}\\rfloor - 1\\ \\le \\lfloor\\frac{s_j}{m}\\rfloor \\le \\lfloor \\frac{s_i}{m} \\rfloor \\}\\tag{5}\nPi​={j:⌊msi​​⌋−1 ≤⌊msj​​⌋≤⌊msi​​⌋}(5)\n这个P~i\\widetilde{\\mathcal{P}}_iPi​表示的是位置i的词能attention to的所有词的集合，其中可能包含了不能attend to的词，需要用mask给去掉。这样一来对于所有的输入向量，我们都能用相同的代码计算attention矩阵了，每个输入向量不同的地方就是mask不同。在实验中，作者设置m=2lnbucketsm=\\frac{2l}{n_{buckets}}m=nbuckets​2l​，每个桶的平均大小为l/nbucketsl/n_{buckets}l/nbuckets​，设置m为其两倍，也就相当于左右各分一半，这里作者假设每个桶大小达到平均大小两倍的概率非常小。下图就是LSH attention的全过程。\n\n首先看左图，第一行是未排序的query=key，然后将每个位置的key分配到一个桶中，每个桶用不同颜色表示。分配完之后先通过桶序号进行排序，得到第三行的结果。然后将这段序列进行分块，每块大小为mmm。最后进行在P~i\\widetilde{\\mathcal{P}}_iPi​内进行attention操作。\n接着是右图，我们看到a图是标准的attention矩阵，非常稀疏但是没有好好利用这些稀疏。b图是经过桶操作并且排序后的结果。c图让Q=K。d图进行分块操作。\n 多轮LSH注意力\n由于hash操作是随机的，难免会出现错误情况，但出错的概率能通过多次hash操作减小。假设我们进行nroundsn_{rounds}nrounds​轮不同的hash操作{h1,h2,h3,…,hnrounds}\\{h^{1},h^{2},h^{3},\\dots,h^{n_{rounds}} \\}{h1,h2,h3,…,hnrounds​}，那么最能得到：\nP~i=∪r=1nroundsPi(r)where P(r)={j:h(r)(qi)=h(r)(qj)}(6)\\widetilde{\\mathcal{P}}_i=\\mathop{\\cup}\\limits_{r=1}^{n_{rounds}}\\mathcal{P}_i^{(r)}\\\\\n\\text{where} \\ \\mathcal{P}^{(r)}=\\{ j:h^{(r)}(q_i)=h^{(r)}(q_j) \\}\\tag{6}\nPi​=r=1∪nrounds​​Pi(r)​where P(r)={j:h(r)(qi​)=h(r)(qj​)}(6)\n 可逆Transformer\n上面的内容解决了Transformer模型的注意力机制复杂度高问题，这里的方法会进一步节省模型的内存消耗。可逆残差网络在上一篇笔记中已经介绍过了。实际上我是为了读懂这里的方法才去看那篇论文的，于是先写了那篇论文笔记作为铺垫。如果没看的话可以先看上一篇论文方法再来继续看这。我就不再解释一遍revnet的方法了，这里实际就直接将revnet的思想运用到了Transformer模型的残差结构中。我将公式再贴一遍，前向传播：\ny1=x1+F(x2)y2=x2+G(y1)(7)y_1=x_1+\\mathcal{F}(x_2)\\\\\ny_2=x_2+\\mathcal{G}(y_1)\\tag{7}\ny1​=x1​+F(x2​)y2​=x2​+G(y1​)(7)\n反向传播：\nx2=y2−G(y1)x1=y1−F(x2)(8)x_2=y_2-\\mathcal{G}(y_1)\\\\\nx_1=y_1-\\mathcal{F}(x_2)\\tag{8}\nx2​=y2​−G(y1​)x1​=y1​−F(x2​)(8)\n将F\\mathcal{F}F换成Attention\\text{Attention}Attention，G\\mathcal{G}G换成FeedForward\\text{FeedForward}FeedForward就得到了可逆的Transformer，作者将归一化层放到了残差结构中去，也就是F,G\\mathcal{F,G}F,G中。\n 分块\n由于前馈网络的每个激活单元是互不影响的，因此将输入的序列分块计算，计算完成后再合起来。假设我们将序列分成了ccc块，那么输出为：\nY2=[Y2(1);… ;Y2(c)]=[X2(1)+FeedForward(Y1(1));… ;X2(c)+FeedForward(Y1(c))](9)Y_2=[Y_2^{(1)};\\dots;Y_2^{(c)}]=[X_2^{(1)}+\\text{FeedForward}(Y_1^{(1)});\\dots;X_2^{(c)}+\\text{FeedForward}(Y_1^{(c)})]\\tag{9}\nY2​=[Y2(1)​;…;Y2(c)​]=[X2(1)​+FeedForward(Y1(1)​);…;X2(c)​+FeedForward(Y1(c)​)](9)\n事实上我感觉这对模型还是有修改的，因为这样就相当于每块共用了前馈层的参数，原始的Transformer每一块的参数是不同的，能表示更多信息，这样做就有信息损失了。\n 总结\n这篇论文结合了Transformer的信息表示优点与RevNet的节省内存的优点以及LSH方法的速度优点，诞生了Reformer。\n","categories":["论文笔记"],"tags":["论文笔记","Transformer"]},{"title":"【论文阅读】The Reversible Residual Network","url":"/2021/12/31/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91The-Reversible-Residual-Network/","content":"原文🔗\n 摘要\n深度残差神经网络推动了图像分类任务的SOTA，并且让又深又宽的网络性能变得更好了。然而内存消耗变成了网络的瓶颈，因为模型需要保存所有的激活单元，以便反向传播时计算梯度。本文提出了Reversible Residual Network（RevNet）,ResNets的一种变体，能将每一层的激活单元通过后面的层构造出来。因此，每一层的绝大多数激活单元都不用存储在内存中，在反向传播的时候可以直接构造出来。本文在多个数据集（CIFAR-10,CIFAR0100,ImageNet）上证明了该方法的分类准确率与同等大小的ResNets是一样的，尽管该方法所需要的用来存储激活单元的内存大小与网络的深度是无关的。\n 介绍\n在过去的几年中，深度卷积网络在多种视觉任务中都取得了快速的进展。对于大部分任务而言，SOTA的网络变得越来越深。比如，深度残差网络的结构在多个视觉领域都取得了SOTA的结果。其结构的关键是残差块（residual block），它使得网络能直接传递信息，并且让反向传播不会出现梯度消失或梯度爆炸。这也使得我们能够训练上百层的网络，而这增加的深度取得了非常好的效果。\n几乎现代的所有神经网络都是用反向传播法训练的。由于反向传播需要存储网络的激活单元，因此它占用的内存是与网络层数成正比的。然而，这就意味着随着网络变得越来越深，越来越宽，存储其中的激活单元将会占用非常多的内存，这也就导致了内存成为许多应用的瓶颈。减少激活单元占用的内存将会大大提高我们训练更深更宽的网络的能力。\n本文提出了一种可逆的残差网络（Reversible Residual Networks，RevNets），ResNets的一种可逆的变体，可逆的含义是，每一层的激活单元能通过之后的可逆层计算出来，这就使得我们不用存储每一层的激活单元在内存中，在方向传播的时候直接计算该位置的激活单元就行。当然，除了少量的不可逆的层需要保存在内存中，不能计算出来。这样设计的结果就是网络需要存储的激活单元的内存大小，是与网络的深度无关的，这通常也直接导致与传统的ResNets相比，网络的大小减少几个数量级。更令人惊讶的是，在作者的实验当中，这样做并没有导致明显的性能下降，几乎与原始的ResNets的效果是一模一样的。\n 背景\n 1.反向传播\n反向传播是计算网络中参数对于损失函数的梯度的经典方法。它几乎被应用于所有的神经网络算法，并且对于具有自动微分功能的神经网络框架来说，它是一个理所应当的存在。由于为了实现节约内存，我们需要手动实现一部分的反向传播，因此先简要介绍回顾一下反向传播。\n我们认为反向传播是一种反向自动求微分的方法。令v1,v2,...vKv_1,v_2,...v_Kv1​,v2​,...vK​代表网络计算图G\\mathcal{G}G中的拓扑顺序的节点，其中vkv_kvk​代表最后的损失函数C\\mathcal{C}C。每一个节点代表的是计算图中的一个函数fif_ifi​。反向传播会计算每个节点的总梯度dC/dvid\\mathcal{C}/dv_idC/dvi​，总梯度表示的是某个节点viv_ivi​微小的变化对于C\\mathcal{C}C的影响，可以想象成导数的定义lim⁡Δx→∞f(x+Δx)−f(x)Δx\\lim\\limits_{\\Delta x\\to\\infty}\\frac{f(x+\\Delta x) - f(x)}{\\Delta x}Δx→∞lim​Δxf(x+Δx)−f(x)​，“总”的意思就是要考虑对viv_ivi​的所有非直接影响，通过它之后的一系列节点vkv_kvk​，通俗一点就是说，对于节点viv_ivi​的梯度，需要将后续节点对其产生的梯度全部加起来。为了避免表示混乱，后续使用vi‾=dC/dvi\\overline{v_i}=d\\mathcal{C}/dv_ivi​​=dC/dvi​来表示对viv_ivi​的总梯度。\n反向传播在计算图上以逆拓扑的顺序进行迭代计算。对于每个节点viv_ivi​，运用如下的求导法则：\nvi‾=∑j∈Child(i)(∂fj∂vi)Tvj‾,(1)\\overline{v_i}=\\sum\\limits_{j\\in \\text{Child(i)}}(\\frac{\\partial f_j}{\\partial v_i})^T \\overline{v_j}, \\tag{1}\nvi​​=j∈Child(i)∑​(∂vi​∂fj​​)Tvj​​,(1)\n其中Child(i)\\text{Child}(i)Child(i)表示viv_ivi​的子节点，∂fj/∂vi\\partial f_j/\\partial v_i∂fj​/∂vi​代表了表达式fjf_jfj​对节点viv_ivi​的雅可比矩阵。上式其实就是简单的链式求导法则，求和代表的就是总梯度，也就是某个节点所有子节点对该节点的偏导数和，我来详细解释一下求和符号内的，对于该节点的导数为何是这样的形式。\n高等数学中我们学的求导都是对一个标量进行求导，也就是自变量是一个数，而不是一个向量。而在深度学习中，我们知道输入模型的xxx通常都是一个向量，非常高维度的向量，经过一个函数之后，输出的yyy也是一个向量，与xxx的维度一致，比如一个简单的线性函数y=wx+by=wx+by=wx+b，参数为w,bw,bw,b，输出输出都是一个向量，这时如果求yyy对xxx的导数，我们得到的就不是一个简单的dy/dxdy/dxdy/dx了，我们得到的是一个雅可比矩阵：\n∂y∂xT=(∂y1∂x1∂y1∂x2⋯∂y1∂xn∂y2∂x1∂y2∂x2⋯∂y2∂xn⋮⋮⋱∂y1∂x1∂yn∂x1∂yn∂x2⋯∂yn∂xn)T\\frac{\\partial y}{\\partial x}^T=\n\\begin{pmatrix}\n\\frac{\\partial y_1}{\\partial x_1} &amp; \\frac{\\partial y_1}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial y_1}{\\partial x_n}\\\\\n\\frac{\\partial y_2}{\\partial x_1} &amp; \\frac{\\partial y_2}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial y_2}{\\partial x_n}\\\\\n\\vdots &amp; \\vdots &amp; \\ddots &amp; \\frac{\\partial y_1}{\\partial x_1}\\\\\n\\frac{\\partial y_n}{\\partial x_1} &amp; \\frac{\\partial y_n}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial y_n}{\\partial x_n}\\\\\n\\end{pmatrix}^T\n∂x∂y​T=⎝⎜⎜⎜⎜⎛​∂x1​∂y1​​∂x1​∂y2​​⋮∂x1​∂yn​​​∂x2​∂y1​​∂x2​∂y2​​⋮∂x2​∂yn​​​⋯⋯⋱⋯​∂xn​∂y1​​∂xn​∂y2​​∂x1​∂y1​​∂xn​∂yn​​​⎠⎟⎟⎟⎟⎞​T\n其中的nnn就是向量的维度。我们将xxx当作求导函数中的viv_ivi​，将yyy当作它的其中一个子节点vjv_jvj​，整个表达式y=wx+by=wx+by=wx+b为两个节点之间的函数fjf_jfj​，那么根据链式求导法则，x‾=y‾dydx\\overline{x} = \\overline{y}\\frac{dy}{dx}x=y​dxdy​，相当于上面求和符号中的某一项，这样看来是不是就非常清晰了，y‾\\overline{y}y​相当于vj‾\\overline{v_j}vj​​。y‾\\overline{y}y​的形式就是：\ny‾=(dy1dy2⋮dyn)\\overline{y}=\n\\begin{pmatrix}\ndy_1\\\\dy_2\\\\\\vdots\\\\dy_n\n\\end{pmatrix}\ny​=⎝⎜⎜⎜⎛​dy1​dy2​⋮dyn​​⎠⎟⎟⎟⎞​\n将上述两个矩阵相乘就能得到x‾\\overline{x}x了。\n 2.深度残差网络\nResNets是由残差块组成的，具有下面的形式：\ny=x+F(x)，(2)y=x+\\mathcal{F}(x)，\\tag{2}\ny=x+F(x)，(2)\n其中F\\mathcal{F}F，残差函数，通常是一个很浅的神经网络。ResNets能很好的解决梯度消失和梯度爆炸问题。由下图所示，分类问题的残差函数是由归一化层（BN），修正线性激活函数层（ReLU）与卷积层堆叠成的（C1代表1×11\\times 11×1的卷积核，C3代表3×33\\times 33×3的卷积核）。\n\n根据ResNets原文说明，我们有两种残差块的结构：基本的残差函数如上图右上部分，具有瓶颈的残差函数如上图的右下部分。何为具有瓶颈结构的卷积层呢？我们知道，一个普通的3×33\\times 33×3卷积层会先将图像进行padding，然后进行卷积，以保证卷积前后图像的大小是不变的。而具有瓶颈结构的残差层，先使用1×11\\times 11×1的卷积核，将图像缩小到某个大小，然后通过普通的3×33\\times 33×3卷积核进行卷积，最后再使用一个1×11\\times 11×1卷积核将图像放大到指定的大小，这样看起来就像是一个瓶颈一样，先大后小再大。上述操作可以通过下面的式子进行表示。\na(x)=ReLU(BN(x))ck(x)=Convk×k(a(x))Basic(x)=c3(c3(x))Bottleneck(x)=c1(c3(c1(x)))(3)a(x)=\\text{ReLU(BN(}x\\text{))}\\\\\nc_k(x)=Conv_{k\\times k}(a(x))\\\\\n\\\\\nBasic(x)=c_3(c_3(x))\\\\\nBottleneck(x)=c_1(c_3(c_1(x)))\\tag{3}\na(x)=ReLU(BN(x))ck​(x)=Convk×k​(a(x))Basic(x)=c3​(c3​(x))Bottleneck(x)=c1​(c3​(c1​(x)))(3)\n 3.可逆结构\n我们先介绍可逆的残差块。可逆的残差网络是由一系列可逆块组成，可逆块的概念类比ResNets的残差块。可逆块的定义如下：\ny_1=x_1\\\\\ny_2=x_2+\\mathcal{F}(x_1)\\tag{4}\\label{eq4}\n\n其中x1,x2x_1,x_2x1​,x2​是每一层输入的拆分，具体怎么拆分需要看使用的是什么方法，是卷积还是普通的全连接网络。\n然而由于上式具有某些性质，不能表达所有的函数，经过改进后的表达式变成了：\ny1=x1y2=x2⊙exp(F(x1))+G(x1)(5)y_1=x_1\\\\\ny_2=x_2\\odot exp(\\mathcal{F}(x_1)) + \\mathcal{G}(x_1)\\tag{5}\ny1​=x1​y2​=x2​⊙exp(F(x1​))+G(x1​)(5)\n其中，⊙\\odot⊙表示的是哈达玛积或者说是点积，也就是每个位置相乘。\n 方法\n下面我们正式介绍可逆残差网络。可逆残差网络就是残差网络的一种变体，能通过后面的层计算当前层的激活单元。我们将要讨论如何在线重构激活单元，以减少存储激活单元需要的内存。\n 1.可逆残差网络\n可逆残差网络是由一系列的可逆残差块组成，我们下面给出定义。我们必须将每一层的单元分割成两部分，分别表示为x1,x2x_1,x_2x1​,x2​；在本文的剩余部分，我们假设这是通过分割通道来完成的，也就是说如果有32个通道，那么分一半当作x1x_1x1​，另一半当作x2x_2x2​，至于为什么这么做呢？因为作者发现这样做效果非常好…\n每一个可逆块都接受一对输入(x1,x2)(x_1,x_2)(x1​,x2​)并且产生一对输出(y1,y2)(y_1,y_2)(y1​,y2​)，根据公式\\eqref{eq4}定义的成对加法规则得到的启发，我们定义如下结构：\ny1=x1+F(x2)y2=x2+G(y1)(6)y_1=x_1+\\mathcal{F}(x_2)\\\\\ny_2=x_2+\\mathcal{G}(y_1)\\tag{6}\ny1​=x1​+F(x2​)y2​=x2​+G(y1​)(6)\n其中函数F,G\\mathcal{F,G}F,G类似于残差结构中的函数，可以自己定义。\n那么，每一层的激活单元可以通过以下式子进行重构：\nx2=y2−G(y1)x1=y1−F(x2)(7)x_2=y_2-\\mathcal{G}(y_1)\\\\\nx_1=y_1-\\mathcal{F}(x_2)\\tag{7}\nx2​=y2​−G(y1​)x1​=y1​−F(x2​)(7)\n需要注意的是，不像残差块，可逆块的卷积步长必须为1，否则会丢失信息，从而导致网络变得不可逆。标准的网络层通常只有少量网络层具有大的步长。如果我们要定义类似于ResNet的结构，那么我们必须显式存储不可逆的网络层。\n\n 2.不存储激活单元的反向传播\n为了深入理解反向传播的程序，我们可以重写一下前向和反向的计算公式，如下：\nforward:z1=x1+F(x2)y2=x2+G(z1)y1=z1backward:z1=y1x2=y2−G(z1)x1=z1−F(x2)(8)forward:z_1=x_1+\\mathcal{F}(x_2)\\quad y_2=x_2+\\mathcal{G}(z_1)\\quad y_1=z_1\\\\\\\\\nbackward:z_1=y_1\\quad x_2=y_2-\\mathcal{G}(z_1)\\quad x_1=z_1-\\mathcal{F}(x_2)\\tag{8}\nforward:z1​=x1​+F(x2​)y2​=x2​+G(z1​)y1​=z1​backward:z1​=y1​x2​=y2​−G(z1​)x1​=z1​−F(x2​)(8)\n尽管z1=y1z_1=y_1z1​=y1​，但两个变量表示计算图中的不同的节点，因此它们两个的总梯度z1‾,y1‾\\overline{z_1},\\overline{y_1}z1​​,y1​​是不一样的。因为z1‾\\overline{z_1}z1​​还受到了y2y_2y2​的间接影响，而y1‾\\overline{y_1}y1​​则不然。在反向传播的过程中，我们有的是激活单元(y1,y2)(y_1,y_2)(y1​,y2​)和它们的总梯度(y1‾,y2‾)(\\overline{y_1},\\overline{y_2})(y1​​,y2​​)，我们希望求出来的是该单元的输入(x1,x2)(x_1, x_2)(x1​,x2​)以及它们的总梯度x1‾,x2‾\\overline{x_1}, \\overline{x_2}x1​​,x2​​，以及函数F,G\\mathcal{F,G}F,G的参数的总梯度。再结合公式(8)，我们就能得到反向传播的更新过程：\n\n怎么理解这个步骤呢？下面我画一张图来帮助理解这个更新步骤。公式的2~4步就不用解释了，就是直接反向求xxx的赋值操作。我们主要解释一下梯度的计算步骤。\n\n上图中黑色的箭头表示的前线传播，紫色箭头表示反向传播。可以看到z1z_1z1​有两个指出的箭头，一个指向y1y_1y1​，一个指向G\\mathcal{G}G，那么反向传播的时候，这两个箭头将会反着对z1z_1z1​产生梯度，我们求的是总梯度，因此需要将两部分梯度相加，得到总梯度。由公式的第5行可以看出来就是这两部分的梯度相加，赋值给z1‾\\overline{z_1}z1​​，∂y2∂z1=y2‾∂g∂z1\\frac{\\partial y_2}{\\partial z_1}=\\overline{y2}\\frac{\\partial g}{\\partial z_1}∂z1​∂y2​​=y2​∂z1​∂g​，这个公式就是这样得到的。x1x_1x1​的总梯度通过上图也能很好的理解，就由读者自己推一下就行了。\n通过重复执行上述的算法，我们就能在只有yyy和y‾\\overline{y}y​的情况下执行反向传播。但总的来说，在一个残差网络里并不是所有层都可逆的，比如下采样层就不可逆，这时候我们需要显示的将激活单元存储在内存中了。但是残差网络通常都包含大量的残差层和少量的下采样层，如果我们模仿经典的残差神经网络来设计结构的话，那少量的下采样层就可以不用在乎了，因为它与网络的深度是无关的。\n 3.关于计算量的问题\n众所周知，鱼和熊掌不可兼得。本方法在节约了内存的同时，一定是会增加计算量的。一般来说，对于一个具有N层的网络，前向传播和方向传播大约分别会进行N次和2N次加乘操作。对于RevNet而言，在方向传播的过程中，残差层的激活单元每次都需要被重新计算一次，因此，RevNet的方向传播大约会进行4N次操作，或者说比普通的残差曾多33%。然而在实验中发现，前向传播和反向传播的开销基本上是一样的，因此在这种情况下，RevNets的计算量更接近50%。（我也不知道是怎么计算出来的）\n\n 总结\n本文主要介绍了RevNets，一种不用存储激活单元的神经网络结构。并且作者发现这种结构在节省内存的同时几乎对网络的性能没有影响。\n","categories":["论文笔记"],"tags":["论文笔记","Resnet","内存优化"]},{"title":"十一月计划及完成情况","url":"/2021/11/30/%E5%8D%81%E4%B8%80%E6%9C%88%E8%AE%A1%E5%88%92%E5%8F%8A%E5%AE%8C%E6%88%90%E6%83%85%E5%86%B5/","content":"11月1日\n\n\n[x] 打毛线\n打了四圈\n\n\n[x] 西瓜书——半监督学习\n计算学习理论没看懂跳过了，半监督学习中的图半监督学习没看懂，其他都大概看懂了\n\n\n[x] 信号与系统继续学习\n\n\n\n11月2日\n\n\n[x] GBDT和XGBoost\nGBDT看懂了，XGBoost看懂了一点，还有很多细节没搞懂，明天也可以继续深挖细节\n\n\n[x] 信号与系统继续学习\n有一点没太学明白，明天要看书复习一遍\n\n\n[x] 跑步\n一公里5分06。。。好垃圾\n\n\n[x] 打毛线\n\n\n\n11月3日\n\n\n[x] XGBoost\n完全疏通，很爽\n\n\n[x] 体测\n跳远2.24m、引体向上3、一千米4：40\n\n\n[ ] 信号与系统复习\n下午体测，晚上继续看了会XGBOOST，就没看，明天开始复习\n\n\n最近还要开始深度学习课程了，先把李宏毅的深度学习看完，代码也要开始练习了，有能力还能看斯坦福的CS224N\n\n11月4日\n\n\n[x] 信号与系统复习\n\n\n[x] 西瓜书——概率图模型\n开了个头，明天继续\n\n\n[x] 打毛线\n\n\n\n11月5日\n\n[x] 隐马尔可夫模型吃透\n[x] 打毛线\n\n\n11月6日\n\n[x] 条件随机场的定义及表示形式\n\n\n11月7日\n\n[x] CRF——最大熵模型\n\n​\t还是很模糊明天看博客\n\n11月8日\n\n\n[x] 最大熵模型与CRF\n最大熵模型基本完成，只是最后的IIS推导还没看明白，CRF最后带入求解也没看懂，下次再看吧\n\n\n\n11月9日\n\n[x] 信号与系统学习+复习\n\n\n11月10日\n\n\n[x] 深度学习——李宏毅\n把HW1重新做了一遍理解了一遍，课程学了一个章节\n\n\n明天要开始复习现代企业管理了，有时间还要再继续改进一下HW1，把误差再降低一些\n\n11月11日\n\n[x] 现代企业管理复习（0~1章）\n\n\n11月12日\n\n[x] 现代企业管理复习（2章）\n\n\n11月13日\n\n\n[x] 现代企业管理复习（3~4章）\n\n\n[ ] 信号与系统学习\n学了三节视频，尽早把第四章学完，还有把ML第一个作业继续调一下，不然总感觉心里有块石头吊着，每天都很不舒服\n\n\n\n11月14日\n\n\n[x] 现代企业管理复习（5章）\n\n\n[x] 信号与系统学习\n\n\n[x] HW1调试\n\n\n\n11月15日\n\n[x] HW1调试\n[ ] 现代企业管理复习\n\n\n11月16日\n\n[x] 现代企业管理复习\n[x] 信号与系统学习\n[x] HW2调试\n\n\n11月17日\n\n[x] 现代企业管理复习\n\n\n11月18日\n\n[x] 现代企业管理复习\n[x] 一拳超人漫画看完\n\n\n11月19日\n\n[x] 现代企业管理复习\n[x] 政审表\n[x] 宝贝作业修改\n[x] 毕业设计询问\n[ ] 打围巾结束\n\n​\t预测失误，1m2根本不够长，还需要继续努力\n\n\n[ ] 宝贝小作文\n写了一半\n\n\n\n11月20日\n\n[x] 形式与政策\n[x] 小作文写信\n\n明天开始继续深度学习，还有毕业论文相关的论文开始看\n\n11月21日\n\n[x] transformer学习\n\n\n11月22日\n\n[x] BERT学习\n[x] 论文阅读\n\n\n11月23日\n\n[x] 论文阅读\n\n\n11月24日\n\n[x] 论文阅读\n\n\n11月25日\n\n[x] 论文阅读\n\n\n11月26日\n\n[x] 组会\n\n\n11月27日\n11月28日\n\n\n[x] 进击的巨人\n\n\n[x] 休息\n\n\n\n11月29日\n\n[x] HW4，attention看完\n\n看了一天的这个作业代码，训练一次要两个小时，根本没办法写。一晚上看pad_sequences没看明白，到宿舍看明白了一些，明天要写一篇博客记录一下，确实有点难理解。还有collate_fn没理解，明天再看几篇博客理解理解。下一步还要看attention源码，pytorch也要多熟悉了。\n\n11月30日\n问题：现在基本上都是在看别人的项目代码，自己还没有试着写过代码，看着别人的代码就觉得很有逻辑，但是如果自己写起来估计就不太写得好，怎么办？\n\n[x] attention is all you need论文阅读\n[x] collate_fn作用\n[x] pytorch的attention源码阅读\n\n​\t看了但没完全看懂，明天再试着继续看一下，还有论文conformer也看看\n\n\n            这个月计划完成的还行，中间有几天看动漫看的着迷了，从早看到玩，需要改进。还有要去锻炼了，体检都说我超重了/(ㄒoㄒ)/~~。下个月要继续pytorch的学习使用，还有李宏毅深度学习看完来。待学习的任务还有CS224N、线性代数基础书、西瓜书很多公式的推导、统计学习方法学习、深度学习代码编写。\n          \n","categories":["计划"],"tags":["计划"]},{"title":"十月计划及完成情况","url":"/2021/11/01/%E5%8D%81%E6%9C%88%E8%AE%A1%E5%88%92%E5%8F%8A%E5%AE%8C%E6%88%90%E6%83%85%E5%86%B5/","content":"10月12日：\n\n\n[x] 西瓜书——决策树\n多变量决策树还未理解\n\n\n[x] 信号与系统学习\n\n\n[x] 跑步三公里\n\n\n\n10月13日：\n\n\n[x] 西瓜书——神经网络\n还剩最后的神经网络分类没看完\n\n\n[x] 西瓜书——LDA\n拉格朗日乘子法还没掌握\n\n\n[ ] 信号与系统学习与复习\nLDA看的久了一些，就没时间来看了\n\n\n[x] 宝贝的信写完\n\n\n\n10月14日：\n\n\n[x] 西瓜书——神经网络\n神经网络结束，朴素贝叶斯看完\n\n\n[x] 信号与系统学复习与学习\n第一章复习完成，第二章学习遇到问题，明天继续啃\n\n\n\n10月15日：\n\n\n[x] 西瓜书——SVM\nSMO算法还没理解透，下次继续理解\n\n\n[x] 拉格朗日乘子法\n\n\n[ ] 信号与系统学习\n\n\n\n10月16日：\n\n\n[x] 西瓜书——贝叶斯分类器\n朴素贝叶斯看完了\n\n\n[x] 信号与系统学习\n\n\n\n10月17日：\n\n\n[x] 西瓜书——贝叶斯分类器\n贝叶斯分类器结束，EM算法学习也结束\n\n\n[ ] 跑步三公里\n\n\n\n10月18日：\n\n[x] 西瓜书——集成学习开头\n[x] EM算法复习理解一下\n[x] 信号与系统学习\n\n\n10月19日：\n\n[x] 西瓜书——集成学习\n[x] 信号与系统复习\n\n\n10月20日：\n\n[ ] 西瓜书——聚类\n[x] 信号与系统学习\n[x] 机器人汇报PPT\n\n\n10月21日：\n\n\n[x] 机器人改进\n机器人优化结束，还剩实验报告没写\n\n\n[x] 信号与系统第二章学习结束\n学习与复习结束，明天开始第三章\n\n\n[x] 西瓜书——聚类\n看了一部分，明天结束\n\n\n\n10月22日：\n\n\n[x] 西瓜书聚类结束\n高斯混合模型和EM算法结束，花了很多时间，导致没时间看信号与系统\n\n\n[ ] 信号与系统第三章开始学习\n\n\n[x] 跑步四公里\n\n\n\n10月23日：\n\n\n[x] 机器人博客撰写\n花了半个上午+一个下午+半个晚上，超出预期\n\n\n[x] 信号与系统第三章开始学习\n刚开了个头，明天继续\n\n\n[ ] 西瓜书——降维\n没空看啦，明天再看吧\n\n\n\n10月24日：\n\n\n[x] 信号与系统继续学习\n学了一点\n\n\n[ ] 西瓜书——降维\n心血来潮更新了我和宝贝的博客，还搭了一个私有云盘花了一天时间，就没搞其他的\n\n\n\n10月25日：\n\n\n[ ] 搭邮件服务器\n一上午没成功。。。\n\n\n其他啥都没干，就当放假\n\n10月26日：\n\n\n[x] 信号与系统第三章学习结束\n\n\n[ ] 西瓜书——降维\n又没完成。。一晚上又去弄WSL去了，明天不能这么下去了，该学什么就学什么，打毛线都还没学，时间要利用起来，不能这么浪费了\n\n\n\n10月27日：\n\n\n[x] 西瓜书——降维，一定要开始了\nKPCA还没看明白，其他的其他的看了一下午，感觉最近的效率有点低了，得调整啊\n\n\n[x] 机器人课设报告要完成了\n一上午写报告\n\n\n[ ] 复习信号与系统第三章\n\n\n[ ] 学打毛线\n\n\n\n10月28日：\n​\t7点33下床\n​\t8点23打开电脑学习\n\n\n[x] 西瓜书——降维\n\n\n[x] 机器人汇报PPT\n\n\n[x] 宝贝的报告写完\n\n\n[x] 复习信号与系统第三章\n\n\n[x] 跑步\n一公里五分零四。。。好拉跨\n\n\n[ ] 学打毛线\n\n\n\n10月29日：\n​\t8点09下床\n​\t十点半开始看机器学习（之前机器人汇报）\n\n\n[x] 西瓜书——特征选择与数据压缩\n特征选择看完了，最后的LASSO回归求解没看懂，数据压缩也基本没看懂\n\n\n[ ] 学打毛线\n\n\n[ ] 信号与系统继续\n\n\n[x] 机器人课设汇报\n\n\n[x] 线性代数视频学习\n\n\n\n10月30日\n​\t7点44下床\n​\t8点40开始学习\n\n\n[x] 线性代数视频学习完成\n\n\n[x] 信号与系统第四章开始学习\n\n\n[x] 打毛线\n打到凌晨两点半，什么都没打出来，打错了\n\n\n\n10月31日\n\n\n[x] 打毛线\n十一点起床打毛线，打到晚上八点半，打错两次什么都没打出来\n\n\n[ ] 西瓜书——计算学习理论\n\n\n\n\n            这个月每天的计划完成情况不是很理想，很多时间都被浪费了，下个月要努力了。\n          \n","categories":["计划"],"tags":["计划"]},{"title":"2021年保研经验贴","url":"/2021/09/21/2021%E5%B9%B4%E4%BF%9D%E7%A0%94%E7%BB%8F%E9%AA%8C%E8%B4%B4/","content":"\n2021年保研大战就快要结束了，感觉形势还是一如既往的严峻，大佬们是一如既往的海。每次面试前，感觉自己复习的挺好，应该没什么问题，可每次面试出来之后，就感觉我怎么什么都不会。在此记录一下我的保研经历，不算非常累，运气起了很大的作用，希望对以后保研的学弟学妹或九推的同学能起到帮助，也欢迎大家在评论区交流！\n\n\n 基本情况\n本科就读于合肥工业大学（不出名的211），所在班级是人数非常少的创新班，夏令营排名为5 / 28，后期综合排名为2 / 28。\n四级：579，六级：596\n论文：无\n竞赛：比较多，但是都挺水的\n项目：一个大创项目，一个实验室项目\n夏令营入营：北京邮电大学计算机学院，华东师范大学计算机学院，山东大学软件学院\n夏令营offer：华师计算机、山大软院（全员优营😅）\n预推免：东南大学计算机、国科大人工智能学院\n最终去向：国科大人工智能学院\n 夏令营\n 北京邮电大学计算机（7.9）\n 概况介绍\n北邮好像是第一次开夏令营，填报的时候系统可以填三个志愿，最后能进一个学院。我至今都没搞清楚是什么情况，系统上显示我是第一个志愿被录取了，但是后来给我面试的好像是第二个志愿，2组软工的，一共入了二十个，好像只要5，6个。刚好那几天没有课，我就去请了一星期的假到外面开宾馆去面试。北邮面试挺简单的，老师们感觉也都很温柔。\n 学院面试\n一上来老师就说面试分为三个部分。\n第一个部分是政治问题，如何看待校园贷、学术不端、当代大学生如何为国家做贡献。说完之后一个老师问我那你说说你如何为国家做贡献，我说我不犯法就是为国家做贡献哈哈哈，然后说了什么可能承担国家的课题之类的，攻克一些技术方面的话。\n第二部分是英语回答问题，那老师中文问问题，要我用英文回答。问了我们学校智能科学与技术专业（我所在的创新班）和计算机科学与技术专业的区别，还有为什么我选择他们的计算机学院不选择人工智能学院。我用的我的中式英语给他们回答了一遍，说我们智科都注重于软件，他们计算机软硬兼顾，我们学了机器学习，计算机视觉还有自然语言处理等，他们注重数据库、操作系统等。然后回答后面一个问题的时候，我最后一句说的是your institute is more … more fit me. 回答完之后我自己都笑了哈哈哈哈。\n第三部分就正式中文面试了。首先中文自我介绍一下，但由于我之前都只背了英文自我介绍，背了好几个晚上的英文版本，背的贼溜，上来居然要我中文自我介绍。我就当场把英文的转成中文的，顺口介绍了一下大创项目。讲完之后发现很多地方没讲到，竞赛还有成绩都没讲到┭┮﹏┭┮然后开始问我的项目，项目的数据是哪来的，项目推荐系统的准确性是如何衡量的（我项目中有个推荐系统），我大创项目的具体流程，然后问我有没有考虑过用步态识别来完成这个项目（前一天听老师的研究生汇报课题，好像他们就研究的步态识别）。然后把我专利问了问，还问了我的比赛情况，我的在比赛中的职责之类的。然后一个老师插了一句，你是智能科学与技术的是吧，那我问你随机森林算法原理是什么。幸好前几天复习机器学习把随机森林给看了一遍，感觉回答的挺好的。最后问我如何看待实习、读研计划、如果研究生遇到困难怎么办。\n 总结\n北邮面试的总体感觉还是不错的，除了英文自我介绍，其他问题回答的都挺好。过了一个星期北邮老师打电话来问我的意向，问我有没有其他的offer，最后去不去北邮。然后我当时非常纠结学硕专硕的区别，我就跟他说如果学硕我就去，专硕不一定。电话打完之后我就后悔了，因为当时回家了一趟还刚进高铁站回校，下午就给老师发了封邮件说：专硕我也愿意！，那老师说：好的！之后就没有消息了。在那个夏令营的群里问出没出结果，大家也都不理我。那个时候拿到华师的offer了，过了几天一气之下就把群给退了🤣\n 山东大学软件学院（7.10 ~ 7.11）\n 概况介绍\n山大软件感觉实力也不错，总共128人面试，好像有些人没参加，最后优营好像96个，几乎全员优营，而且优营之后还没说是否能直接录取，说的是当前保研名额还没确定，等九月份才能确定是否录取。然后九月份的时候居然还开了预推免，真不知道山大要搞什么。\n 10号宣讲会\n宣讲会就是各个实验室老师汇报他们做的东西，做的情况，非常无聊，而且还会记录时长，不能退出，进去和结束都要签到，好烦人。我就把腾讯会议在那挂着，一下都没听，去车站接女朋友去了。从上午讲到下午，上午八点半开始，下午六点多结束，中间一小时休息，其他时间一直在讲。。\n 11号面试\n感觉山大面试面不出个什么东西来，又不要我英文自我介绍，让我中文自我介绍。然后英文问了我项目几个问题，我都回答上了。然后那些老师就一直问我项目，这个怎么做的，有没有考虑什么什么什么。我就说没考虑，没时间做那么多。然后就结束了。\n 总结\n山大夏令营之后要联系导师，不联系导师不给优营，而且还发了个问卷挺唬人，有一项就是拿到优营之后去不去山大，填不去那肯定不要你，填去，那心里又不舒服，最后还是填了去哈哈哈哈。九月份的时候又收集了一个问卷，我就直接填了不去。感觉山大面试问不出什么水平来，完全就问项目，专业课专业知识什么的一下都没问。\n 华东师范大学计算机（7.13 ~ 7.15）\n 概况介绍\n入了华师计算机是我意想不到的，里面也有很多运气成分。华师夏令营的系统可以填2个志愿，我五月份在华师夏令营网站看到计算机学院要求专业排名前15%，而我是17%，所以我当时就没去报计算机夏令营。因为软件工程要求前20%就行，所以我第一志愿就填了软件工程，推荐信用的是我大创指导老师的推荐信。然后到了六月多，系统快关了，我再去官网看了看，发现计算机的排名要求也变成了20%，我就又赶紧报了个计算机学院，然后我想两个推荐信用同一个老师不太好，我就换了个之前我大数据比赛的指导老师的推荐信。最后没想到第一志愿没入，第二志愿给入了。计算机夏令营总共入了114人，实际进群100个左右，后来有的人又退群，说不去了，还剩九十多个人在群里。到机试的时候看到只有83个人提交过，最后优营了60个，比例还是非常高的，入了营基本上就稳了。\n 12号宣讲会\n第一天就是一大堆的宣讲，各个团队的情况介绍啦，还有各个老师的课题汇报之类的。我基本没怎么听，在吃东西跟女朋友一起看觉醒年代哈哈哈。就听了两个比较感兴趣的团队，一个机器学习团队，一个语言认知与知识计算团队。\n 13号上午机考 + 下午交流\n第二天就是机考以及与各个团队的交流。之前就听说华师夏令营的机考是他们学校的ACM出题，所以非常慌，一直也在练算法。用的当然就是华师自己的OJ，感兴趣的可以注册一个去看看，刷刷题ECNU Online Judge。账号是当天早上考前发的，中途不能掉线，因为账号绑定IP地址，如果掉线IP变了这个号就登不上了。一共四题，每题100分，感觉挺难的，没有人AC但有人拿了360，真的是太牛了。我最后拿了230分，感觉有个十几二十名吧。\n\n A. 索引查询\n\n B. 框体排序\n\n C. 放水\n\n\n\n D. 正则表达式匹配\n\n 14号团队面试\n14号下午需要提交一个志愿表，要填写打算去的团队，以及一二三志愿导师，学院会尽量根据志愿情况，让志愿上的团队来面试你。我当时提交的团队是机器学习团队，因此第二天面试我的就是机器学习团队的老师。\n首先英文自我介绍（终于能用到自我介绍了），我就把背了无数遍的自我介绍又熟练的背了一遍。然后是给了一段英文文章，让我读一遍，然后翻译出来。我感觉我读的还行，翻译的时候就有点磕磕绊绊，自己知道是什么意思，估计面试的老师没听明白我在说什么。翻译完之后还有一个老师问我六级过了没(─.─|||\n自我介绍之后给了一张图片，让我回答图片上的问题，第一个问题是容器和虚拟机的概念，当时一下没反应过来容器是什么，就说我没听说过容器。面试结束后想了想，容器不就是docker嘛，之前学Java的时候我还用过，居然没回答上。第二个问题是SVM是什么，让我讲讲SVM的原理之类的。由于我当时就不是很了解SVM，只知道这个SVM是最大分类间隔分类器，前几天又正好看了李宏毅老师的SVM讲解，就说了一大堆没说到重点，把那个老师都说懵了。我说了什么就是最大分类间隔，然后用的损失函数是hinge-loss之类的，那老师说不是问你损失函数，问你原理。我又说了一大堆屁话哈哈哈，然后这个问题就结束了。\n之后就是各个老师问问题，一个老师问看你做这个项目，对于训练模型方面你有什么经验可以给我们分享吗，有没有遇到什么坑。由于我的项目都是随便做了做，根本没去训练，当时想要老师给台服务器训练模型，老师都不给我，直接让学长给了我一个训练好的模型，然后我就在那胡乱跟老师扯哈哈哈，说到什么学习率方面。然后他又问那学习率方面你怎么做的，我说我用的学习率衰减。他继续问学习率衰减还有什么其他方法吗，我回答说按照当前损失函数梯度设置学习率，梯度越小学习率设置越小，需要计算当前梯度之类的。然后那老师说叫我去看看Pytorch的学习率衰减方法，里面有很多，我这时候才反应过来他问的是Pytorch里的学习率衰减策略，我之前也是有看过的啊喂，只是不知道他问的是这个。\n总的来说，面试感觉很凉，很凉很凉，面完之后就一直躺床上，一直在想该怎么回答。\n 总结\n面试完第二天，那个填了志愿的老师就打电话给我了，问了问我的情况，说我面试的分数怎么这么低，问了我什么问题，我说问了我SVM，然后他说，哦，我记得你。当时刚好就是这个老师一直在问我SVM，把他给说蒙了。。。然后他说他打电话问了我老师（给我写推荐信的）我的情况，由于我比赛给我们学校那个老师挂了好几个名，然后都拿到了奖，所以这个老师给我说了一通好话，把我推荐给这老师了，而且好巧不巧，这个老师跟华师我填志愿的老师之前就有合作，很早就认识了，他才有机会跟我说好话。然后华师的老师最后说可以要我，把我简历发他一份。刚跟那老师打完电话，我们学校的老师就给我打电话了，说华师老师打电话问他我的情况，说我面试成绩不好，然后他帮我解释说我不太会面试，但是实践能力很强，比赛拿了很多奖，对我印象很深刻之类的话。我当时真是太感谢他了！这老师真的太好了呜呜呜。\n过了两个星期出结果，我也顺利的拿到了优营。在这还是要感谢一下我的老师呜呜呜。\n 预推免\n 东南大学计算机学院\n 概况介绍\n东南计算机总共入营了四五百人，计算机学院和软件学院好像是一起面试的，分了好多组并行面试。东南是夏令营和预推免一起开的，所以入了很多人，而且东南入营了没有任何短信或邮件通知，只有加了群或者经常关注官网或系统才能知道自己入营了，感觉很多同学就因为这个错过了入营的机会，没去系统里点确认参加。由于当时已经拿到华师offer了，也不想去东南，就没复习天天在家玩了。所以最后面试情况也不好，没拿到优营。\n 分组面试（8.7）\n我面试是第40号，总共46人，到下午才轮到我。首先是讲PPT，学院发了个PPT模版，要自己做个PPT然后屏幕共享讲解。讲完之后英文问我项目，我做了那些工作，以及这个项目怎么做的之类的。很久没有面试，没有练口语了，所以回答的超级烂，磕磕绊绊，没有一句话完整说完。英文问完之后老师开始问问题，第一个老师说我项目太简单了。第二个老师看我PPT最后一页写了大数据之类的什么什么，就问我大数据方面的问题：1、如果数据量太大，比方1T或更大，单机无法全部加载，你怎么训练模型，2、数据量如果太少，你有什么办法解决。我当时一个都没答好，第一个问题我就没回答，老师说不用都会打，我就回答了第二个，我说数据加强，自己手动做数据什么的。然后那个老师说这类方法的名字叫什么你知不知道，我又说不知道。然后就结束了。\n 总结\n面试完之后我就好迷茫，怎么问我一个都不会，心里有点难受呜呜呜。然后去网上查了一下，看到感觉那些方法都不是老师想要问的方法。总的来说自己还是非常菜，还是要继续提升自己，多看看书。\n 国科大人工智能学院\n 概况介绍\n我在七月份还是八月份的时候就在国科大系统上报名了预推免，然后一直没有消息，我还以为我早就凉了，前两个星期晚上跑步的时候，北京来了一个电话，我一看北京，我还以为北邮被鸽穿了来找我了哈哈哈哈，没想到是国科大的老师来问我情况，问我有没有别的offer，我说有个上海的offer，但如果国科大能要我我还是想冲一下，去面试。了解了一下我的情况后就说过几天面试，等消息。过了两天官网就发通知了，总共19人入了，加群的有16人，最终就16个人面试，不知道招多少人，按往年情况来看，估计招十个左右。进群看到一个清华，一个上交，压力就大起来了。\n 学院面试\n国科大学院面试感觉也很舒服，老师看起来也都很友好，首先是英文自我介绍，我还是用的之前背的模版，但这次综合排名出来之后，我又多加了两句话说我已经取得了保研资格，并且综合排名是第二名。之后老师问我创新班是什么情况，让我介绍一下。又问了一下我项目的情况，以及一些课程情况。因为我简历上写能熟练运用C++，Python，Java进行编程，老师就问有多熟练，到哪种程度。最后问我如何在不知道python列表情况下获得最后一个元素。。。总的来说非常简单，就跟聊天一样。\n 导师面试\n第二天上午我就打电话给填了志愿的老师问我能不能去，她说她在开会，下午再联系我。下午她就发封邮件给我，让我看一篇论文，第二天一起讨论讨论。那篇论文是关于transformer变形的论文，我就先把transformer看了一遍，然后去看这篇论文，当天晚上就看的差不多了。第二天起来又看了一会。下午五点的时候来给我面试，一共三个人，两个老师，还有一个人应该是她的研一学生。讲完之后她说我讲的不错，一天时间能理解到这个程度很不错。第二天早上我刚醒没一会，她就打电话来说要我了，把我报到学院去了。然后我的面试就这么结束了。\n 总结\n国科大从通知我面试到最后拟录取我，正好一个星期，12号老师打电话了解情况，19号就给我发了拟录取通知，都没太反应过来。去网上搜了一下国科大人工智能学院的信息，都不太多，因为是2017年才新成立的学院。然后经过多方打听，并在拟录取我的那个中午，我也跟导师聊了三四十分钟，把我想知道的信息都问清楚了，感觉这个学院还是不错的，而且导师感觉也很不错~~，比华师找的那个好多了~~至少不会很差吧，而且听我在北京读书的朋友说国科大在北京名气挺大的，刚好我女朋友毕业以后也去北京，我就选择了国科大人工智能学院，把华师给拒了。\n\n 写在最后\n2021年保研就快要结束了，我的去向也基本上定了下来，虽然还不知道国科大究竟怎么样，但终究只是走的路不一样，关键还是要看研究生阶段自己的造化了。去哪里其实都一样，但求个无悔二字，只要自己满意就行。等我去那边读书了再来更新那边的情况吧。\n保研大战刚开始的时候，我去博客上看了很多经验贴，南大，中科大厦大之类的，我就感觉如果我去我也行，可实际上到自己报名的时候，连入营的机会都没有。连续被拒十多次，心态都要爆炸了。后来心态慢慢也放平了，看到保研群里的大佬那么多，要不就是都有论文，要不就是排名特别高，我这种没论文没排名的人，那些学校确实看不上我，我也不做那种春秋大梦了。\n\n今年保研的情况不会比上一届严峻，海王依旧很多，往往十个人就能占了一百个夏令营名额，所以按道理来说预推免会有很多的机会。但实际上今年很多学校都特别海，候补的人特别多，基本上入营后，除了优营的选手就是候补。这也就导致我后来的第二名的排名没什么用了，想去投，但学校不多了。本来非常想去中科大，但中科大今年收我们学校的人不多，往年入营的人都有很多，今年却卡了rank，只收了我们班前两名，和计算机的前三名，后面的人压根就没机会，而且由于WL很长，我现在有rank了但没机会给我投了。\n最后也希望没有offer的同学在九推中拿到满意的offer，学弟学妹保研的时候也能拿到心仪的offer！\n\n11月15日补充\n在开系统最后的那几天，我身边有很多同学给中科大的老师发邮件，中科大很多老师都没招满，都需要自己去联系，很多同学就通过自己联系去了中科大，联系其他学校的也有，所以到最后快开系统的几天机会还是有很多的，海王放了offer就有机会了。所以不要轻易放弃，不要因为夏令营失利就否定自己！\n","categories":["保研"],"tags":["保研","计算机","夏令营","面试"]},{"title":"PX4无人机+Gazebo仿真实现移动物体的跟踪","url":"/2021/10/23/PX4%E6%97%A0%E4%BA%BA%E6%9C%BA-Gazebo%E4%BB%BF%E7%9C%9F%E5%AE%9E%E7%8E%B0%E7%A7%BB%E5%8A%A8%E7%89%A9%E4%BD%93%E7%9A%84%E8%B7%9F%E8%B8%AA/","content":"\n            这个学期我们有一个智能机器人系统的课设，我们组分配到的题目是《仿真环境下使用无人机及相机跟踪移动物体》，本文主要记录完成该课设的步骤以及内容。我们采用的最终方案是PX4飞控+gazebo仿真+mavros通讯控制，实现了在gazebo环境下无人机跟踪一个移动的小车。本文所使用的是Ubuntu18.04 + melodic。\n          \n\n 试验环境介绍\n\n            首先要搞懂各个部分的关系[7]，以及各自的作用，才能对控制无人机有个完整的认识，我在一开始做的时候就花了很多时间都没搞懂PX4到底是个无人机还是个什么东西，mavros又是干什么的。下面我简要介绍一下各个部分的关系，让大家有个大致的了解。\n          \n PX4飞控\nPX4是一个飞控固件，所谓的飞控固件，就是能够向无人机发出控制命令，控制无人机的位姿、飞行速度以及螺旋桨的转速等等。无人机的运动就需要通过飞控固件发出命令来控制。官网的用户手册在这，推荐看英文版本，中文版本有的地方翻译的实在是太烂了，我看的时候感觉像是机翻的，而且与原文的位置都不太一样。\n Gazebo仿真\ngazebo仿真就不用多说了吧，在学ros基本的操作的时候就应该接触过gazebo。这就是一个能够模拟现实世界的仿真软件，PX4的源代码里就提供了PX4无人机的gazebo模型，通过launch文件直接运行就能得到一个gazebo下的无人机。\n MAVROS通讯\nmavros里面有个ros，一看就是和ros相关的。我们看官网的介绍*“MAVROS – MAVLink extendable communication node for ROS with proxy for Ground Control Station.”*，这句话的意思是，MAVROS是MAVLink为了让ROS代理控制站的扩展交流节点。首先MAVLink是一个无人机通讯协议，也就是说与无人机交流所发出的信号或数据格式都要符合该协议，与HTTP等协议是一个道理。然后控制站其实是PX4为了控制无人机所开发的一个图形化控制站，可以通过GUI的形式来操作无人机，给一般用户很好的体验。而这里是用来代理控制站，也就是说充当控制站来控制无人机。到这里就很明显了，MAVROS就相当于代码版的控制站，若想要通过ros节点开控制无人机的飞行，那就必须通过mavros这个包，在这个包内包含了控制无人机的消息格式等。\n\n            综上所述，整个无人机的控制逻辑就是，通过mavros向PX4飞控发送控制命令，PX4再将命令发送到无人机的各个组件，以控制无人机按照用户的逻辑进行运动。而该无人机就在gazebo中，在gazebo中可以看到无人机的运动情况。\n          \n 实验过程\n PX4无人机的安装\n 1、安装环境依赖\nsudo apt install -y ninja-build exiftool python-argparse python-empy python-toml python-numpy python-yaml python-dev python-pip ninja-build protobuf-compiler libeigen3-dev genromfs xmlstarlet libgstreamer1.0-dev libgstreamer-plugins-base1.0-dev\n 2、安装python依赖（melodic默认的是python2）\npip install pandas jinja2 pyserial cerberus pyulog numpy toml pyquaternion  -i https://pypi.tuna.tsinghua.edu.cn/simple\n我安装的时候pyulog没装上，其他的都装上了，如果你们也有这个问题，就把其他的装上，pyulog后面还有个步骤会自动安装\n 3、安装ros与gazebo\n这两个的安装就不多说了，如果没安装的话可以在网上先安装好这两个再继续操作。\n 4、安装mavros\nsudo apt install ros-melodic-mavros ros-melodic-mavros-extraswget https://gitee.com/robin_shaun/XTDrone/raw/master/sitl_config/mavros/install_geographiclib_datasets.shsudo chmod a+x ./install_geographiclib_datasets.shsudo ./install_geographiclib_datasets.sh #这步需要装一段时间,请耐心等待PX4配置\n\n            我在安装的时候出现两个问题。第一个问题是sudo apt install包的时候一直出现404，未找到这个包，解决方案时sudo apt update，将软件源更新一下，很可能是原始的位置已经过期了，需要更新才能找到最新的位置。第二个问题是最后一步的.sh文件执行太慢了，我挂那两三个小时都没结束，大概是因为被墙了吧。解决办法如下[8]：在 /usr/share 下目录新建 GeographicLib 目录。将  geoids gravity magnetic 三个文件夹拷贝到 /usr/share/GeographicLib 文件夹下面。上述三个文件夹的链接在这，提取码：dje9\n          \n 5、PX4安装\n这里推荐使用gitee安装，非常感谢XTDrone团队[2]将代码放在了gitee上，我也使用过github安装，但总是断开连接，无数次重试才完整安装完成。\ncd ~git clone https://gitee.com/robin_shaun/PX4_Firmwarecd PX4_Firmwaregit checkout -b xtdrone/dev v1.11.0-beta1bash ./Tools/setup/ubuntu.sh --no-nuttx --no-sim-tools\n将.gitmodules替换为如下内容（在PX4_Firmware文件夹中ctrl+h查看隐藏文件）[1]\n[submodule &quot;mavlink/include/mavlink/v2.0&quot;]\tpath = mavlink/include/mavlink/v2.0\turl = https://gitee.com/robin_shaun/c_library_v2.git\tbranch = master[submodule &quot;src/drivers/uavcan/libuavcan&quot;]\tpath = src/drivers/uavcan/libuavcan\turl = https://gitee.com/robin_shaun/uavcan.git\tbranch = px4[submodule &quot;Tools/jMAVSim&quot;]\tpath = Tools/jMAVSim\turl = https://gitee.com/robin_shaun/jMAVSim.git\tbranch = master[submodule &quot;Tools/sitl_gazebo&quot;]\tpath = Tools/sitl_gazebo\turl = https://gitee.com/robin_shaun/sitl_gazebo.git\tbranch = master[submodule &quot;src/lib/matrix&quot;]\tpath = src/lib/matrix\turl = https://gitee.com/robin_shaun/Matrix.git\tbranch = master[submodule &quot;src/lib/ecl&quot;]\tpath = src/lib/ecl\turl = https://gitee.com/robin_shaun/ecl.git\tbranch = master[submodule &quot;boards/atlflight/cmake_hexagon&quot;]\tpath = boards/atlflight/cmake_hexagon\turl = https://gitee.com/robin_shaun/cmake_hexagon.git\tbranch = px4[submodule &quot;src/drivers/gps/devices&quot;]\tpath = src/drivers/gps/devices\turl = https://gitee.com/robin_shaun/GpsDrivers.git\tbranch = master[submodule &quot;src/modules/micrortps_bridge/micro-CDR&quot;]\tpath = src/modules/micrortps_bridge/micro-CDR\turl = https://gitee.com/robin_shaun/micro-CDR.git\tbranch = px4[submodule &quot;platforms/nuttx/NuttX/nuttx&quot;]\tpath = platforms/nuttx/NuttX/nuttx\turl = https://gitee.com/robin_shaun/NuttX.git\tbranch = px4_firmware_nuttx-9.1.0+[submodule &quot;platforms/nuttx/NuttX/apps&quot;]\tpath = platforms/nuttx/NuttX/apps\turl = https://gitee.com/robin_shaun/NuttX-apps.git\tbranch = px4_firmware_nuttx-9.1.0+[submodule &quot;platforms/qurt/dspal&quot;]\tpath = platforms/qurt/dspal\turl = https://gitee.com/robin_shaun/dspal.git[submodule &quot;Tools/flightgear_bridge&quot;]\tpath = Tools/flightgear_bridge\turl = https://gitee.com/robin_shaun/PX4-FlightGear-Bridge.git\tbranch = master [submodule &quot;Tools/jsbsim_bridge&quot;]\tpath = Tools/jsbsim_bridge\turl = https://gitee.com/robin_shaun/px4-jsbsim-bridge.git[submodule &quot;src/examples/gyro_fft/CMSIS_5&quot;]\tpath = src/examples/gyro_fft/CMSIS_5\turl = https://gitee.com/mirrors/CMSIS_5\n再次执行子模块更新指令\ngit submodule update --init --recursive\n编译\nmake px4_sitl_default gazebo\n配置环境变量，注意路径的匹配，你若修改了文件夹名要进行对应的修改，第一个catkin_ws是自己的工作目录。\nsource ~/catkin_ws/devel/setup.bashsource ~/PX4_Firmware/Tools/setup_gazebo.bash ~/PX4_Firmware/ ~/PX4_Firmware/build/px4_sitl_defaultexport ROS_PACKAGE_PATH=$ROS_PACKAGE_PATH:~/PX4_Firmwareexport ROS_PACKAGE_PATH=$ROS_PACKAGE_PATH:~/PX4_Firmware/Tools/sitl_gazebo\n下面我们就可以测试PX4无人机了，执行下面的命令\ncd ~/PX4_Firmwareroslaunch px4 mavros_posix_sitl.launch\n此时会打开gazebo环境，里面地面上有一个无人机。\n\n最后一步，测试无人机通讯\nrostopic echo /mavros/state\n若显示的消息中出现connected: True,则说明MAVROS与SITL通信成功。到此，无人机的配置就结束了。\n 移动小车的安装\n\n            由于实验要求的是实现移动物体的跟踪，因此我使用了一个可控制的小车来代替移动物体[10]，通过键盘控制节点可以控制小车在gazebo环境中移动。\n          \n本试验使用的是TurtleBot3小车，安装和控制移动都非常方便。\n安装小车命令[5]\nsudo apt-get install ros-melodic-turtlebot3-*\n通过上述命令就安装好了TurtleBot小车，是不是很方便。\n由于该小车有三种形态，所以还需要通过环境变量指定一种形态，否则无法运行，我实验中使用的是burger形态，其他两种形态你们可以自己去修改，我下面都以burger形态小车来讲解。通过环境变量指定小车有一下两种方式，推荐第二种一劳永逸，但若要修改就需要进入.bashrc文件中修改\nexport TURTLEBOT3_MODEL=burger\t\t\t\t\t\t\t# 每次打开新的终端都要执行echo &quot;export TURTLEBOT3_MODEL=burger&quot; &gt;&gt; ~/.bashrc\t\t#直接写入环境变量，打开终端每次都会自动执行\n下面我们运行小车，测试一下小车控制\nroslaunch turtlebot3_gazebo turtlebot3_world.launch\n然后运行键盘控制节点\nrosrun teleop_twist_keyboard teleop_twist_keyboard.py\n若没安装该节点需要先安装，执行下面的命令\nsudo apt-get install ros-melodic-teleop-twist-keyboard\n\n通过I L J K ,四个键可以控制小车的运动，详细的运动控制自己看控制台的输出。到此，移动小车的安装就已经完成。\n PX4无人机添加摄像头以及配置的修改\n\n            通过上面的工作，我们完成了无人机和移动物体的配置，若要完成无人机通过相机追踪小车，那相机怎么能少得了呢？默认的PX4无人机是不带摄像头的，我们需要修改配置文件使其带上一个摄像头。\n          \n给PX4添加一个深度摄像机[3]\ncd ~/PX4_Firmware/launchcp mavros_posix_sitl.launch mavros_posix_sitl_cp.launch\t\t# 不修改原始无人机文件，复制一个副本进行修改gedit mavros_posix_sitl_cp.launch\n\n            我后面的修改操作都是基于副本，先复制一份然后操作副本，以保持源代码的结构\n          \n做如下改动\n添加\n&lt;arg name=&quot;my_model&quot; default=&quot;iris_downward_depth_camera&quot;/&gt;\n修改\n&lt;arg name=&quot;sdf&quot; default=&quot;$(find mavlink_sitl_gazebo)/models/$(arg vehicle)/$(arg vehicle).sdf&quot;/&gt; \n为\n&lt;arg name=&quot;sdf&quot; default=&quot;$(find mavlink_sitl_gazebo)/models/$(arg vehicle)/$(arg my_model).sdf&quot;/&gt; \n\n            注意添加的代码需要在修改的上方\n          \n添加摄像头就完成了，但是该摄像头默认的分辨率是48 * 64，非常低，飞高一些就看不清地面的小车了，我们还需要修改一下摄像头的分辨率。\ncd ~/PX4_Firmware/Tools/sitl_gazebo/models/depth_cameragedit depth_camera.sdf\n将对应部分修改为\n&lt;update_rate&gt;10&lt;/update_rate&gt;...&lt;image&gt;&lt;format&gt;R8G8B8&lt;/format&gt;&lt;width&gt;400&lt;/width&gt;&lt;height&gt;400&lt;/height&gt;&lt;/image&gt;\nwidth与height对应的就是摄像头的分辨率，update_rate是图像的发布频率，由于把像素改高了，怕系统处理速度慢，因此把频率降低一倍。\n下面我们来测试一下摄像头是否配置成功。\ncd ~/PX4_Firmwareroslaunch px4 mavros_posix_sitl_cp.launch\t\t# 注意我修改的都是副本，不要运行错了\n放大无人机可以看见前面装上了一个长条形摄像机，这就是一个向下的深度相机。\n然后打开rviz，新开一个终端输入\nrviz\n先添加一个接收图像的窗口\n\n将图像的话题选择为/camera/rgb/image_raw，另一个对应的是深度图像，可以自己切换看看效果。\n\n这个时候image窗口就会显示无人机摄像机拍摄下来的图像，然后在运行无人机的那个终端输入命令commander takeoff，观察该图像窗口，会随着无人机起飞变化。\n\n由于这个地面平坦，将小车放到这个环境中的话，小车会动不了，所以要将仿真环境改一下，改成原始的空仿真环境。\nPX4仿真环境的配置文件是/home/ljw/PX4_Firmware/launch/posix_sitl.launch，与之前一样，我们不对原始文件进行修改，我们修改副本。\ncd ~/PX4_Firmware/launchcp posix_sitl.launch posix_sitl_cp.launchgedit posix_sitl_cp.launch\n将\n&lt;!-- Gazebo sim --&gt;&lt;include file=&quot;$(find gazebo_ros)/launch/empty_world.launch&quot;&gt;\t&lt;arg name=&quot;gui&quot; value=&quot;$(arg gui)&quot;/&gt;\t&lt;arg name=&quot;world_name&quot; value=&quot;$(arg world)&quot;/&gt;\t&lt;arg name=&quot;debug&quot; value=&quot;$(arg debug)&quot;/&gt;\t&lt;arg name=&quot;verbose&quot; value=&quot;$(arg verbose)&quot;/&gt;\t&lt;arg name=&quot;paused&quot; value=&quot;$(arg paused)&quot;/&gt;\t&lt;arg name=&quot;respawn_gazebo&quot; value=&quot;$(arg respawn_gazebo)&quot;/&gt;&lt;/include&gt;\n修改为\n&lt;!-- Gazebo sim --&gt;&lt;include file=&quot;$(find gazebo_ros)/launch/empty_world.launch&quot;&gt;    &lt;arg name=&quot;gui&quot; value=&quot;$(arg gui)&quot;/&gt;    &lt;arg name=&quot;world_name&quot; value=&quot;$(find turtlebot3_gazebo)/worlds/empty.world&quot;/&gt;    &lt;arg name=&quot;debug&quot; value=&quot;$(arg debug)&quot;/&gt;    &lt;arg name=&quot;verbose&quot; value=&quot;$(arg verbose)&quot;/&gt;    &lt;arg name=&quot;paused&quot; value=&quot;$(arg paused)&quot;/&gt;    &lt;arg name=&quot;respawn_gazebo&quot; value=&quot;$(arg respawn_gazebo)&quot;/&gt;&lt;/include&gt;\n也就是将原本的gazebo的.world文件换成turtlebot3小车的empty.world文件，这个world里什么都没有。光这样修改还没有生效，因为我们修改的是副本，原始调用这个文件的文件也需要修改，调用这个文件的文件就是之前的mavros_posix_sitl_cp.launch\ngedit mavros_posix_sitl_cp.launch\n将\n&lt;include file=&quot;$(find px4)/launch/posix_sitl.launch&quot;&gt;\n修改为\n&lt;include file=&quot;$(find px4)/launch/posix_sitl_cp.launch&quot;&gt;\n到此，无人机的配置就已经完成，可以再次运行一次无人机，看看环境是否发生变化。\n 合并无人机和移动小车\n\n            在前面的步骤中，我们将无人机和移动小车都准备好了，下面我们就只要将无人机和小车放在同一环境中，就能开始我们的跟踪实验了。\n          \n通过阅读turtlebot3的启动文件/opt/ros/melodic/share/turtlebot3_gazebo/turtlebot3_empty_world.launch，可以看到启动小车的代码，我们将该代码添加到启动无人机的文件中，就能同时启动小车和无人机。\ncd ~/PX4_Firmware/launchgedit posix_sitl_cp.launch\n添加如下代码\n&lt;!-- car model and parameter --&gt;    &lt;arg name=&quot;model&quot; default=&quot;$(env TURTLEBOT3_MODEL)&quot; doc=&quot;model type [burger, waffle, waffle_pi]&quot;/&gt;    &lt;arg name=&quot;x_pos&quot; default=&quot;1.0&quot;/&gt;    &lt;arg name=&quot;y_pos&quot; default=&quot;1.0&quot;/&gt;    &lt;arg name=&quot;z_pos&quot; default=&quot;0.0&quot;/&gt;    &lt;param name=&quot;robot_description&quot; command=&quot;$(find xacro)/xacro --inorder $(find turtlebot3_description)/urdf/turtlebot3_burger.urdf.xacro&quot; /&gt;&lt;!-- gazebo car model --&gt;&lt;node pkg=&quot;gazebo_ros&quot; type=&quot;spawn_model&quot; name=&quot;spawn_urdf&quot; args=&quot;-urdf -model turtlebot3_$(arg model) -x $(arg x_pos) -y $(arg y_pos) -z $(arg z_pos) -param robot_description&quot; /&gt;\n我们让小车出生在1 1位置，无人机默认出生在0 0位置。为了让小车更容易被无人机识别，我们将小车全身改成黑色。\nroscd turtlebot3_descriptioncd urdfsudo gedit turtlebot3_burger.gazebo.xacro\n\n            注意选择自己的小车文件进行修改，我这里修改的是burger小车的。\n          \n将文件中所有&lt;material&gt;Gazebo/xxx&lt;/material&gt;中的xxx全部改成Black。\n到此，无人机与移动小车全部配置完毕执行，我们测试一下。\ncd ~/PX4_Firmwareroslaunch px4 mavros_posix_sitl_cp.launch\n\n我们可以看到中间一个无人机和一个黑乎乎的小车，周围的环境已经变成了空的了。然后启动键盘控制节点可以控制小车的运动\nrosrun teleop_twist_keyboard teleop_twist_keyboard.py\n 控制无人机运动\n通过阅读PX4官网的一个控制实例[9]，我们可以大致了解无人机运动的控制流程。我将实例代码复制过来并且添加注释，让大家对无人机控制代码有个理解。\n/*头文件，包括常见的geometry_msgs和mavros通信需要的mavros_msgs，添加上就行*/#include &lt;ros/ros.h&gt;#include &lt;geometry_msgs/PoseStamped.h&gt;#include &lt;mavros_msgs/CommandBool.h&gt;#include &lt;mavros_msgs/SetMode.h&gt;#include &lt;mavros_msgs/State.h&gt;/*current_state表示的是无人机的状态，在主函数中订阅了对应的话题，这个状态就会不断更新，表示无人机当前的状态。state_cb就是对应的回调函数，会不断的执行，更新状态。现在先不用管为什么需要这个状态，后面的代码会解释。*/mavros_msgs::State current_state;void state_cb(const mavros_msgs::State::ConstPtr&amp; msg)&#123;    current_state = *msg;&#125;int main(int argc, char **argv)&#123;    ros::init(argc, argv, &quot;offb_node&quot;);    ros::NodeHandle nh;    \t// 订阅无人机的状态    ros::Subscriber state_sub = nh.subscribe&lt;mavros_msgs::State&gt;            (&quot;mavros/state&quot;, 10, state_cb);        /*     发布一个geometry_msgs::PoseStamped的消息，需要知道的是，这个消息是控制无人机的一种方式，将指定坐标包裹进这个消息，然后发布出去，无人机就能自动飞行到指定的坐标地点    */     ros::Publisher local_pos_pub = nh.advertise&lt;geometry_msgs::PoseStamped&gt;            (&quot;mavros/setpoint_position/local&quot;, 10);        /*    无人机有一个锁，如果不解锁，无人机虽然接受了命令但是不会动被锁住了，只有解锁了才能对无人机进行控制，下面这个服务调用就是用来请求解锁无人机。上面的current_state就包含了无人机是否解锁的信息，若没解锁就需要解锁，否则就不用，其用途在这就体现出来    */    ros::ServiceClient arming_client = nh.serviceClient&lt;mavros_msgs::CommandBool&gt;            (&quot;mavros/cmd/arming&quot;);        /*    无人机飞行有很多种模式，如果需要用代码操控无人机，我们就需要切换到OFFBOARD模式。上面的current_state也包含了无人机当前的飞行模式，若不是OFFBOARD就需要切换到该模式。下面的这个服务调用就是用来请求切换无人机飞行模式。    */    ros::ServiceClient set_mode_client = nh.serviceClient&lt;mavros_msgs::SetMode&gt;            (&quot;mavros/set_mode&quot;);    //the setpoint publishing rate MUST be faster than 2Hz    /*    在OFFBOARD模式下，需要以&gt;=2HZ的频率向无人机发送消息，否则无人机会回退到OFFBOARD模式之前所在的模式，因此这里的rate需要设置的比2大就行    */    ros::Rate rate(20.0);\t    // wait for FCU connection    /*    等待无人机与控制站连接（代码的方式就是代理），只有连接了才能发送消息    */    while(ros::ok() &amp;&amp; !current_state.connected)&#123;        ros::spinOnce();        rate.sleep();    &#125;    /*    pose就是坐标，本实例是让无人机在2m处悬空，因此z设置为2，z表示的就是高度    */    geometry_msgs::PoseStamped pose;    pose.pose.position.x = 0;    pose.pose.position.y = 0;    pose.pose.position.z = 2;    // 下面这个感觉有没有都无所谓    //send a few setpoints before starting    for(int i = 100; ros::ok() &amp;&amp; i &gt; 0; --i)&#123;        local_pos_pub.publish(pose);        ros::spinOnce();        rate.sleep();    &#125;    // 请求的切换模式的消息，设置为OFFBOARD    mavros_msgs::SetMode offb_set_mode;    offb_set_mode.request.custom_mode = &quot;OFFBOARD&quot;;    // 请求解锁的消息，arm表示解锁，设置为true，disarm是上锁    mavros_msgs::CommandBool arm_cmd;    arm_cmd.request.value = true;    // 记录上次请求的时间    ros::Time last_request = ros::Time::now();    while(ros::ok())&#123;        // 如果无人机模式不是OFFBOARD并且离上次操作时间大于5秒就发送请求切换，这里的5s是为了演示清楚设置的延时        if( current_state.mode != &quot;OFFBOARD&quot; &amp;&amp;            (ros::Time::now() - last_request &gt; ros::Duration(5.0)))&#123;            if( set_mode_client.call(offb_set_mode) &amp;&amp;                offb_set_mode.response.mode_sent)&#123;                ROS_INFO(&quot;Offboard enabled&quot;);            &#125;            // 更新本次请求的时间            last_request = ros::Time::now();        &#125; else &#123;            // 如果当前未解锁且与请求时间大于5s，就发送请求解锁            if( !current_state.armed &amp;&amp;                (ros::Time::now() - last_request &gt; ros::Duration(5.0)))&#123;                if( arming_client.call(arm_cmd) &amp;&amp;                    arm_cmd.response.success)&#123;                    ROS_INFO(&quot;Vehicle armed&quot;);                &#125;                last_request = ros::Time::now();            &#125;        &#125;\t\t        // 不断发送位置消息，但是只有解锁后才能真正开始运动，如果不发送就会退出OFFBOARD模式，因为请求发送速度要&gt;=2HZ        local_pos_pub.publish(pose);        ros::spinOnce();        rate.sleep();    &#125;    return 0;&#125;\n通过官网的代码就能知道控制无人机飞行的一般流程，只要将核心代码逻辑修改一下，就能实现无人机对小车的跟踪了。可以先将上面的代码运行一次，观察一下无人机的运动。\n\n            总体流程为：1、通过launch文件启动无人机的gazebo环境2、新建一个工作空间，包含的依赖有roscpp、std_msgs、geometry_msgs、mavros、cv_bridge、image_transport、sensor_msgs其中后面几个是为了后续图像处理用的，这里一并导入3、创建一个cpp文件，将上述代码复制进去4、修改CMakeLists5、rosrun该节点在无人机解锁后就能看到无人机飞到了空中2m处\n          \n 控制无人机跟踪运动小车\n\n            通过上述无人机悬空的代码我们已经了解了控制无人机飞行的代码流程：首先定义好需要发送与接收的话题消息，并且定义好各个请求，然后将无人机切换到OFFBOARD模式，接着解锁无人机，同时需要一直给无人机发送运动控制的消息，包括位置控制或速度控制，并且频率要大于2HZ。通过以上流程框架，我们就能设计一个自动跟踪移动小车的代码。\n          \n通过网上查阅资料看到，控制无人机运动不仅可以通过发送位置消息[4]，还可以像控制小乌龟一样，发送速度消息[6]，这就为跟踪小车的提供了方案。我设计的跟踪思路：\n之前案例订阅和发布的话题就不用多说了，全部都要订阅，然后需要额外订阅的是无人机发送的图像，之前设置了10HZ，也就是一秒钟无人机会发送十帧图像过来，在该订阅的回调函数内，对图像进行处理，检查图像内是否有小车，如果有的话，就通过比较小车像素点的位置和图像中心像素点的位置，来判断方位，并相应的设置速度。图像处理回调函数如下：\n#include &lt;ros/ros.h&gt;#include &lt;geometry_msgs/PoseStamped.h&gt;#include &lt;geometry_msgs/Twist.h&gt;#include &lt;mavros_msgs/CommandBool.h&gt;#include &lt;mavros_msgs/SetMode.h&gt;#include &lt;mavros_msgs/State.h&gt;#include &lt;mavros_msgs/Altitude.h&gt;#include &quot;sensor_msgs/Image.h&quot;#include &quot;cv_bridge/cv_bridge.h&quot;#include&lt;opencv2/core/core.hpp&gt;#include&lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/imgproc/imgproc.hpp&gt;// 一些全局变量// 悬空高度（追踪小车的高度）const double h = 4;// 调整高度的速度（上升或下降）const double hv = 0.1;// 控制无人机的速度geometry_msgs::Twist velocity;// 无人机当前的高度double curH;// 无人机是否已经稳定在空中的标志bool start = false;void doImg(const sensor_msgs::Image::ConstPtr &amp;msg) &#123;        if(!start) return;        // 将无人机发布的图像先转化为灰度图，再进行二值化，就能得到黑白图像，若小车出现，那么在图像内有黑色的像素，否则图像全是白色像素，这也是我将小车改成黑色的原因，若改成其它颜色就不好进行分离    cv_bridge::CvImagePtr cv_ptr = cv_bridge::toCvCopy(msg, sensor_msgs::image_encodings::BGR8);\tcv::Mat img = cv_ptr -&gt; image;    cv::Mat gray, bin;    cv::cvtColor(img, gray, cv::COLOR_BGR2GRAY);    cv::threshold(gray, bin, 127, 255, cv::THRESH_BINARY);        // 获得图像的宽和高    static int row = bin.rows, col = bin.cols;    // 图像中心点的位置，我们假设图像中心点的位置就是无人机的位置，这样就能很方便的发布速度来控制无人机    static double centX = row / 2, centY = col / 2;        // x y用来记录小车在该帧图像出现的位置    int x, y;    // 是否找到小车的标记    bool findCar = false;        // 遍历图像，若图像内有黑色像素则代表发现了小车，记录下此时的x y位置    for(int i = 0; i &lt; row; i++) &#123;        for(int j = 0; j &lt; col; j++) &#123;            uchar point = bin.at&lt;uchar&gt;(i, j);            if(point == 0) &#123;                findCar = true;                x = i, y = j;                break;            &#125;        &#125;        if(findCar) break;    &#125;        // 记录最后一次找到小车的时间    static ros::Time last_find_time = ros::Time::now();    if(findCar) &#123;        ROS_INFO(&quot;找到目标位置, x = %d, y = %d&quot;, x, y);        // 将小车（所在像素点）相对无人机（图像中心像素点）的位置归一化到0 ~ 1之间，并以此作为控制无人机的速度，小车离无人机越远，则无人机的速度越大，否则无人机的速度越小        double vx = abs(centX - x) / centX;        double vy = abs(centY - y) / centY;                // 经测试，无人机发送的图像的垂直方向是无人机的x方向，图像的水平方向是无人机的y方向        // 因此，若小车（像素位置）在无人机（像素位置）上方，需要发送一个正的x方向速度，否则要发送一个负方向的速度        if(x &lt; centX) velocity.linear.x = vx;        else velocity.linear.x = -vx;        \t\t// y方向同理        if(y &lt; centY) velocity.linear.y = vy;        else velocity.linear.y = -vy;        // 若不给无人机发送z方向的速度，无人机会时上时下，因此通过下面这个代码控制无人机高度，若低于一定高度，就发布z方向的速度，若高于某个高度，就发送一个-z方向的速度，让无人机下降        if(curH &lt; h - 0.5) velocity.linear.z = hv;        else if(curH &lt; h + 0.5) velocity.linear.z = 0;        else velocity.linear.z = (curH - h) * -hv;        ROS_INFO(&quot;发布速度 x : %f, y : %f, z : %f&quot;, velocity.linear.x, velocity.linear.y, velocity.linear.z);        // 记录无人机最后一次发现小车的时间，后面有用        last_find_time = ros::Time::now();    &#125; else &#123;        ros::Time now = ros::Time::now();        velocity.linear.x = 0;        velocity.linear.y = 0;        // 无人机丢失目标五秒内，什么都不操作        if(now - last_find_time &lt; ros::Duration(5)) &#123;            ROS_INFO(&quot;没有找到目标...&quot;);        &#125; else &#123;            // 无人机丢失目标五秒后，开始向上飞行（扩大视野）来搜寻小车，搜寻的最高高度是无人机跟踪小车高度的两倍，这也是前面代码中控制无人机下降的原因，若无人机在升空过程中发现目标小车，会立刻下降跟踪小车            if(curH &lt; 2 * h - 1) &#123;                ROS_INFO(&quot;上升高度寻找，当前高度为：%.2f&quot;, curH);                velocity.linear.z = hv;            &#125; else &#123;                if(curH &gt; 2 * h + 1) velocity.linear.z = -hv;                else velocity.linear.z = 0;                ROS_INFO(&quot;目标丢失。。。&quot;);            &#125;        &#125;    &#125;&#125;\n上面的回调函数完成了对无人机追踪小车速度的控制，其运行逻辑是：若无人机发现了小车，就通过小车相对无人机的方位，发送x y方向的速度，否则如果丢失的话，在五秒内不进行任何操作，超过五秒后，开始提升无人机的高度扩大视野来寻找小车，最大高度是跟踪小车高度的两倍，一旦发现小车立刻下降并跟踪小车。\n上面只是一个回调函数，还要主函数来控制无人机。\nvoid do_H(const mavros_msgs::Altitude::ConstPtr&amp; msg) &#123;    curH = msg-&gt;local;&#125;mavros_msgs::State current_state;void state_cb(const mavros_msgs::State::ConstPtr&amp; msg)&#123;    current_state = *msg;&#125;int main(int argc, char **argv)&#123;    ros::init(argc, argv, &quot;offb_node&quot;);    ros::NodeHandle nh;    setlocale(LC_ALL, &quot;&quot;);    ros::Subscriber state_sub = nh.subscribe&lt;mavros_msgs::State&gt;            (&quot;mavros/state&quot;, 10, state_cb);    ros::Publisher local_pos_pub = nh.advertise&lt;geometry_msgs::PoseStamped&gt;            (&quot;mavros/setpoint_position/local&quot;, 10);    ros::Publisher local_vec_pub = nh.advertise&lt;geometry_msgs::Twist&gt;            (&quot;/mavros/setpoint_velocity/cmd_vel_unstamped&quot;, 10);    ros::ServiceClient arming_client = nh.serviceClient&lt;mavros_msgs::CommandBool&gt;            (&quot;mavros/cmd/arming&quot;);    ros::ServiceClient set_mode_client = nh.serviceClient&lt;mavros_msgs::SetMode&gt;            (&quot;mavros/set_mode&quot;);    ros::Subscriber img_sub = nh.subscribe&lt;sensor_msgs::Image&gt;(&quot;/camera/rgb/image_raw&quot;, 10, doImg);    ros::Subscriber height_sub = nh.subscribe&lt;mavros_msgs::Altitude&gt;            (&quot;/mavros/altitude&quot;, 10, do_H);    //the setpoint publishing rate MUST be faster than 2Hz    ros::Rate rate(20.0);    // wait for FCU connection    while(ros::ok() &amp;&amp; !current_state.connected)&#123;        ros::spinOnce();        rate.sleep();    &#125;    geometry_msgs::PoseStamped pose;    pose.pose.position.x = 0;    pose.pose.position.y = 0;    pose.pose.position.z = h;    velocity.linear.x = 0;    velocity.linear.y = 0;    velocity.linear.z = 0;    mavros_msgs::SetMode offb_set_mode;    offb_set_mode.request.custom_mode = &quot;OFFBOARD&quot;;    mavros_msgs::CommandBool arm_cmd;    arm_cmd.request.value = true;    ros::Time last_request = ros::Time::now();    bool takeoff = false;    while(ros::ok())&#123;        if(!takeoff) &#123;            if( current_state.mode != &quot;OFFBOARD&quot; &amp;&amp;                (ros::Time::now() - last_request &gt; ros::Duration(2.0)))&#123;                if( set_mode_client.call(offb_set_mode) &amp;&amp;                    offb_set_mode.response.mode_sent)&#123;                    ROS_INFO(&quot;Offboard enabled&quot;);                &#125;                last_request = ros::Time::now();            &#125;            if( !current_state.armed &amp;&amp;                (ros::Time::now() - last_request &gt; ros::Duration(2.0)))&#123;                if( arming_client.call(arm_cmd) &amp;&amp;                    arm_cmd.response.success)&#123;                    ROS_INFO(&quot;Vehicle armed&quot;);                &#125;                last_request = ros::Time::now();            &#125;            if( current_state.armed &amp;&amp;                 (ros::Time::now() - last_request &gt; ros::Duration(5.0))) &#123;                    takeoff = true;                    ROS_INFO(&quot;Vehicle stabled&quot;);                    start = true;                    ROS_INFO(&quot;开始追踪...&quot;);                    last_request = ros::Time::now();                &#125;            local_pos_pub.publish(pose);        &#125; else &#123;            local_vec_pub.publish(velocity);        &#125;        ros::spinOnce();        rate.sleep();    &#125;    return 0;&#125;\n上述代码的逻辑比较好理解，我就不加注释，流程是：先通过位置控制无人机，让无人机在高度h处稳定悬空，当无人机稳定悬停在空中时，将控制无人机的方式改为速度控制，也就是通过发送速度来控制无人机。\n\n            为了方便讲解，将代码分成了两部分贴，将两部分合起来放在一个cpp里，就能正常执行。代码中无人机是否稳定悬空是通过一个时间延迟实现的，假定无人机能在五秒内悬停在指定点，五秒前都是通过位置控制无人机，五秒后就一直通过速度控制无人机。\n          \n到此，无人机跟踪运动小车的整个实验就完成了。\n 演示视频\n \n 参考\nUbuntu18.04下基于ROS和PX4的无人机仿真平台的基础配置搭建\n ↩XTDrone仿真平台基础配置\n ↩PX4+gazebo仿真给无人机添加摄像头\n ↩使用ROS节点控制PX4——位置控制\n ↩ROS-melodic学习turtlebot3笔记＜一功能包导入与测试＞\n ↩PX4学习笔记3: 速度控制\n ↩APM,PX4,GAZEBO,MAVLINK,MAVROS,ROS之间的关系以及科研设备选型\n ↩执行 install_geographiclib_datasets.sh 错误！\n ↩MAVROS Offboard control example\n ↩在gazebo中导入移动小车+二维码\n ↩autolabor ROSTutorials\n ↩","categories":["ROS"],"tags":["ROS","PX4","目标跟踪"]}]